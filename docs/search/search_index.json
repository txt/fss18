{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Foundations of Software Science Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. Graduates of this class will becomes producers , not mere consumers , of AI software. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus . Why? Facetiously, I'll say this subject asks: \"If deep learning is the answer, what was the question?\". In this subject, we ask \"what kind of learners do we need?\" then work backwards from there to design new AI methods, or improve existing ones. This is an important task since the following is a very pressing issue: The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject. What? How to teach software how to be a scientist In short, we will teach software how to a scientist; i.e. how to automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI. How? Monthly: Can you do better than last year's high water mark ? A 500 to 1000-fold speed up of a text miner? August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"Home"},{"location":"#foundations-of-software-science","text":"Fall 2018 NC State Computer Science Tuesdays, Thursdays, 4:30pm Prof. Tim Menzies Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. Graduates of this class will becomes producers , not mere consumers , of AI software. This subject will explore methods for generating and using models built from data (i.e. how to collectthat data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects). For more admin details, see the syllabus .","title":"Foundations of Software Science"},{"location":"#why","text":"Facetiously, I'll say this subject asks: \"If deep learning is the answer, what was the question?\". In this subject, we ask \"what kind of learners do we need?\" then work backwards from there to design new AI methods, or improve existing ones. This is an important task since the following is a very pressing issue: The future of SE is more and more AI . But AI software is still software; So as society uses more AI, SE folks will be required to use and maintain and extend that software; So how should SE people look at AI software? What should they expect from that software? What would be an AI software \"bad smells\" that prompts a code reorganization? Enter this subject.","title":"Why?"},{"location":"#what","text":"How to teach software how to be a scientist In short, we will teach software how to a scientist; i.e. how to automatically build, critique, and revise models. How to build and modify and refactor and remix and repurpose: Software for data miners; Software for optimizers; Software for theorem provers. To help society, and our customers, achieve their dreams better, faster, and cheaper. How to achieve maximum AI benefits at minimal AI costs: By recognize and modify bias in learners. By reducing the CPU and memory footprints of AI; By respecting the privacy of citizens in a society shared with AI.","title":"What?"},{"location":"#how","text":"Monthly: Can you do better than last year's high water mark ? A 500 to 1000-fold speed up of a text miner? August = Start-up September = Data mining October = Optimizers and theorem provers November = Your do your own big project that utilizes the tools and perspectivs of this sibject. Weekly (till mid-October): Small, simple programming assignments Regular poster presentations by students on some tiny aspect of AI+SE For full details, see the syllabus .","title":"How?"},{"location":"about/","text":"Neque iam est venti flamma corpora aderam Consulit cunctae cruore inposuit Quis superi Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit. Dum mens populos intrarant at genusque multaque Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit Eosdem mutata A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#neque-iam-est-venti-flamma-corpora-aderam","text":"","title":"Neque iam est venti flamma corpora aderam"},{"location":"about/#consulit-cunctae-cruore-inposuit-quis-superi","text":"Lorem markdownum; si muta , iam vidi. Gravet crevit, afuerunt aetherias datus, gravitate utque; amnemque breve et excutiant . Scit vineta vincas dentes tympana. Imagine me mediis sequentis nati coloni illa distabat? Que sed creavit terreris flammaeque ipsos cornuaque ponto iugulo formaeque Seditioque naves sors temptanti. Servaberis Cererem alis, est sua his divam euntem, opemque. Debes vectus , ait, ordine , pallentemque valeant respicere, et ego. Hanc illo favilla promissis ut habenis Tritoniaca: sex ad matres habet , distinxit.","title":"Consulit cunctae cruore inposuit Quis superi"},{"location":"about/#dum-mens-populos-intrarant-at-genusque-multaque","text":"Non fuit ipse, vim sed Erectheus sequitur: contra et corymbis. Cadit pars vitulus , erat anno inmiti momordit mihi, sparserat sacravere haut concussit miles. Puro una , est figura celer. Neque virum poterat Sunt studiumque audit iam Dubitat natus caret poteram suspicere Dea templis sede alimenta obscenique feroces vidit","title":"Dum mens populos intrarant at genusque multaque"},{"location":"about/#eosdem-mutata","text":"A pudori nemorum loquentem, si sic relicta, flammas violentaque matrem. Tamen omnia nec cernitis mugitus popularia paterno formas flava sed? Inferius haec et Alce sacerdos, imago unde dea ab tacitus dixi . redundancy_ccd_menu.fileWysiwygWpa(url, directory_floppy_meta - toggle, led( tiffToggleScrolling, printer)); tableTroubleshootingAlu(syntax, server_aiff); leopard_bios += c_jpeg(computing_unfriend_led(fileRosettaUp, winsock)); var dvdStationBluetooth = font(wiredDriver.hot_json_dithering(secondary, key, pim_bluetooth_bit) / 3); Visa fecit, esse Circes omnibus forma : Minervae expulit cum colat quiescere. Ignotis status mutatum, Haemus , videntur, femineo intrarunt de absunt iubentque turba inritamina suae. Quae neci non has aurigam plangorem quae, sensit frustra molitur laceri, alendi madidus quattuor. Romulus tum , qui non ab movent conligit Coronida, nostra; et.","title":"Eosdem mutata"},{"location":"history/","text":"SE + AI: then and now SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject SE: the past e.g. \"SE is not about requirements engineering\" (which is wrong) e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993 e.g. \"Programmning is not about testing\" (wrong again) e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team e.g. \"Programming is not about deploying software\" (so very, very wrong) Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles Q: So What's next? A: AI SE: the present Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest. SE: the future Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE+AI,  then and now &#10004;"},{"location":"history/#se-ai-then-and-now","text":"SE's past is full of cases where someone declared \"X\" was not part of SE then we ignored them and added \"X\" to SE and lots of things got lots better So the question I pose to you is this: Q: What is currently \"not\" SE, but soon must be? A: AI Enter this subject","title":"SE + AI: then and now"},{"location":"history/#se-the-past","text":"","title":"SE: the past"},{"location":"history/#eg-se-is-not-about-requirements-engineering-which-is-wrong","text":"e.g. From Boehm, Keynote, 2004 , slide 8: \"The notion of 'user' cannot be precisely defined, and therefore has no place in CS or SE.\" Edsger Dijkstra, ICSE 4, 1979 \"Analysis and allocation of the system requirements is not the responsibility of the SE group but is a prerequisite for their work.\" Mark Paulk at al., SEI Software CMM* v.1.1, 1993","title":"e.g. \"SE is not about requirements engineering\" (which is wrong)"},{"location":"history/#eg-programmning-is-not-about-testing-wrong-again","text":"e.g. Harlin Mills, 1984 : software engineers should write, but not run or test, their own software \"Cleanroom software engineering\" No unit testing (instead, mathematical verification) so before we run anything, we write perfect code And there is a seperate testing team to the programming team","title":"e.g. \"Programmning  is not about testing\" (wrong again)"},{"location":"history/#eg-programming-is-not-about-deploying-software-so-very-very-wrong","text":"Before devops, the coding team used to hand off the system to the production team Now we do much less of that.. achieving must faster change cycles","title":"e.g. \"Programming  is not about deploying software\"  (so very, very wrong)"},{"location":"history/#q-so-whats-next","text":"A: AI","title":"Q: So What's next?"},{"location":"history/#se-the-present","text":"Software now mediates what we see and how we act Chemists win Nobel Prize for software sims Engineers use software to design optical tweezers, radiation therapy, remote sensing, chip design Web analysts use software to analyze clickstreams to improve sales and marketing strategies Stock traders write software to simulate trading strategies Analysts write software to mine labor statistics data to review proposed gov policies Journalists use software to analyze economic data, make visualizations of their news stories Etc etc etc In short, now more than ever, software really really matters In London or New York, The time for ambulance to reach patient is controlled by models If you cross the border Arizona to Mexico, Models determine if you are taken away for extra security measures If you default on a car loans, Models determine when (or if) someone repossesses your car Autonomous cars Software is essential to international financial and transport systems; our energy generation and distribution systems; and even the pacemakers that control the beat of our hearts. Looking forward, to the forthcoming age of autonomous cars and flying drones, it is clear that software models (written in traditional programming languages or in some next-generation interpretation) will be key in determining what we can do, when, where, and how. So how can we help our AI systems reason better about our data, and our models? Using data mining, we might learn a model from data that predicts for (say) a single target class; Using optionzers, we might a multi-objective optimizer to find what solutions score best on multiple target variables. Also, data miners can be used to to summarize the data, after which optimizers can leap to better solutions, faster ; Also, optimizers can be used to select intelligent settings for data mining algorithms e.g. such as how many trees should be included in a random forest.","title":"SE: the present"},{"location":"history/#se-the-future","text":"Software enginenering isn't just about software any more Olde SE: just polish up the lens of the telescope New SE (with AI): use the telescope to look and understand and change \"things\" After \"continuous integration\" (where we automated everything) Comes \"AI everywhere\" (where we automate automation). From Software Analytics: What\u2019s Next? , IEEE Software, Sept/Oct 2018: \"Consider the rise of the data scientist in industry. Many organizations now pay handsomely to hire large teams of data scientists. For example, at the time of this writing, there are more than 1,000 Microsoft employees exploring project data using software analytics. These teams are performing tasks that a decade ago would have been called cutting-edge research. But now we call that work standard operating procedure .\" \"Every innovation also offers new opportunities. There is a flow-on effect from software analytics to other AI tasks outside of software engineering. Software analytics lets software engineers learn about AI techniques, all the while practicing on domains they understand (i.e., their own development practices). Once developers can apply data-mining algorithms to their data, they can build and ship innovative AI tools. While sometimes those tools solve software engineering problems (e.g., recommending what to change in source code), they can also be used on a wider range of problems. That is, we see software analytics as the training ground for the next generation of AI-literate software engineers working on applications such as image recognition, large-scale text mining, autonomous cars, drones, etc.\" \"What is the most important technology newcomers should learn to make themselves better at data science (in general) and software analytics (in particular)? \"To answer this question, we need a workable definition of \u201cscience,\u201d which we take to mean a community of people collecting, curating, and critiquing a set of ideas. In this community, everyone does each other the courtesy to try to prove this shared pool of ideas. By this definition, most data science (and much software analytics) is not science. Many developers use software analytics tools to produce conclusions, and that\u2019s the end of the story. Those conclusions are not registered and monitored. There is nothing that checks whether old conclusions are now out of date (e.g., using anomaly detectors). There are no incremental revisions that seek minimal changes when updating old ideas. \"If software analytics really wants to be called a science, then it needs to be more than just a way to make conclusions about the present. Any scientist will tell you that all ideas should be checked, rechecked, and incrementally revised. Data science methods such as software analytics should be a tool for assisting in complex discussions about ongoing issues. Which is a long-winded way of saying that the technology we most need to better understand software analytics and data science is ... science.\"","title":"SE: the future"},{"location":"inspiration/","text":"Inspiration Light the fire Learn why the world wags and what wags it Live for the surprise \u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Inspiration &#10004;"},{"location":"inspiration/#inspiration","text":"","title":"Inspiration"},{"location":"inspiration/#light-the-fire","text":"","title":"Light the fire"},{"location":"inspiration/#learn-why-the-world-wags-and-what-wags-it","text":"","title":"Learn why the world wags and what wags it"},{"location":"inspiration/#live-for-the-surprise","text":"\u201cIf the world merely lived up to our wildest dreams, what a dull place it would be. Happily\u2026\u201d \u2013 Tim Menzies","title":"Live for the surprise"},{"location":"license/","text":"License Tim Menzies 2018 This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"(c) 2018"},{"location":"license/#license","text":"Tim Menzies 2018 This work is licensed under a Creative Commons Attribution 4.0 International License You are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: Notice: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"syllabus/","text":"Syllabus Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232 Synopsis Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects. Objectives By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper. Staff Lecturer Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:30 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email). Teaching assistant TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD Group mailing list During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above. Prerequisite Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed. Suggested texts none Expected Workload Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above). Grades The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: No homeworks will be accepted after October 15. Once you submit homework i+1, your marks for homework 1..i will freeze. Only 2 homeworks (max) will be marked per student per week (this includes resubmissions) You lose 0.5 marks for any week where you do not submit a homework (including resubmissions) Mid-term Oct 28 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the submit site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implementations An initial implementation which you will critize using our baseline criteria A second implementation where the you strive to improve that implementation, according to any of the baseline criteria. Note that merely doing some existing standard data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Lecturer may call students/ teams at any time to his office for such reviews. Presentations will start mid-November Paper is not due till Dec 1. Note that: Master students will do the big project in groups of 3. All other work is solo. Submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting. Attendance Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements. Academic Integrity Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted. Disabilities Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average). Non-Discrimination Policy NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148. Other Information Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Syllabus &#10004;"},{"location":"syllabus/#syllabus","text":"Foundations of software science NCSU, CS, Fall 2018 CSC 591-023 (16403) CSC 791-023 (17051) Tues/Thurs 4:30 to 5:45 EE III, Room 2232","title":"Syllabus"},{"location":"syllabus/#synopsis","text":"Most software companies now learn their policies via data-driven methods. Modern practitioners treat every planned feature as an experiment, of which only a few are expected to survive. Key performance metrics are carefully monitored and analyzed to judge the progress of a feature. Even simple design decisions such as the color of a link are chosen by the outcome of software experiments. This subject will explore methods for designing data collection experiments; collecting that data; exploring that data; then presenting that data in such a way to support business-level decision making for software projects.","title":"Synopsis"},{"location":"syllabus/#objectives","text":"By the end of the course, students should be able to Build and modify and refactor and remix and repurpose AI software (data miners, optimizers, theorem provers); Report on complex technical issues (spoken talk) Report on complex technical issues (written talk) Discuss issues associated with applying the above to help society, and customers, achieve their goals better, faster, and cheaper.","title":"Objectives"},{"location":"syllabus/#staff","text":"","title":"Staff"},{"location":"syllabus/#lecturer","text":"Tim Menzies (Prof) Office Hours: Tuesday, 2:00-4:30 and by request Location of Office Hours: EE II room 3298 Slack name: timm E-Mail: timm@ieee.org Only use this email for private matters. All other class communication should be via the class Slack group http://found18.slack.com . Phone: 304-376-2859 Do not use this number, except in the most dire of circumstances (best way to contact me is via email).","title":"Lecturer"},{"location":"syllabus/#teaching-assistant","text":"TBD Office Hours: TBD Location of Office Hours: TBD Slack name: TBD","title":"Teaching assistant"},{"location":"syllabus/#group-mailing-list","text":"During term time, all communication will be via the Slack group https://found18.slack.com. . Students are strongly encouraged to contribute their questions and answers to that shared resource. + Note that, for communication of a more private nature, contact the lecturer on the email shown above.","title":"Group mailing list"},{"location":"syllabus/#prerequisite","text":"Note that this is a programming-intensive subject. A programming background is required in a contemporary language such as Java or C/C++ or Python. Hence,he prerequisite for this class is 510, Software Engineering. Significant software industry experience may be substituted, at the instructor\u2019s discretion. Students in this class will work in Python, but no background knowledge of that language will be assumed.","title":"Prerequisite"},{"location":"syllabus/#suggested-texts","text":"none","title":"Suggested texts"},{"location":"syllabus/#expected-workload","text":"Sometimes, the lecturer/tutor will require you to attend a review session during their consultation time. There, students may be asked to review code, concepts, or comment on the structure of the course. Those sessions are mandatory and failure to attend will result in marks being deducted. Also, this is tools-based subject and it is required that students learn and use those tools (Python, repositories, etc). Students MUST be prepared to dedicate AT LEAST 5-8 working hours a week to this class (excluding the time spent in the classroom). Laboratory instruction is not included in this subject (but the first three weeks will be spent on some in-depth programming tutorials). Note that the workload for masters and Ph.D. students will be different (see above).","title":"Expected Workload"},{"location":"syllabus/#grades","text":"The following grade scale will be used: A+ (97-100), A (93-96), A-(90-92) B+ (87-89), B (83-86), B-(80-82) C+ (77-79), C (73-76), C-(70-72) D+ (67-69), D (63-66), D-(60-62) F (below 60). Grades will be added together using: Weekly solo homeworks (till October): 8 marks (8 homeworks 1 mark each) Homeworks can be resubmitted till you get full marks on them. But: No homeworks will be accepted after October 15. Once you submit homework i+1, your marks for homework 1..i will freeze. Only 2 homeworks (max) will be marked per student per week (this includes resubmissions) You lose 0.5 marks for any week where you do not submit a homework (including resubmissions) Mid-term Oct 28 (on terminology): 22 marks Solo poster (on current directions in foundations of software science): 10 marks 2 page paper Posters are 2 pages pdf in ACM format Sample posters can be found here Topic: Posters will explain some interesting aspect on some paper on some work NOT authored by Menzies. Something on data mining and/or search-based SE and/or thorem proving as applied to some SE task To find examples of that work, see papers after 2012 from top SE-venues Google scholar, top SE-venues In particular, ICSE, TSE, JSS, IST, MSR, ESE, ASE, TOSEM, ICSM (now ICSME) Poster may burrow definitions and graphics from the paper they are reviewing. BUT. Anything the student must be able to explain in further detail anything they put into their poster. Presentation (5 mins talk, 5 mins questions) 10 marks No slides Instead, place your 2 page paper under the podium camera, then talk to the content. IMPORTANT : Prior to class, post your paper to the submit site . Otherwise, lose 1 mark. Big project (due end November) on automated SE): 25 (essay) 15 (presentation) 10 (code review) Project must present two implementations An initial implementation which you will critize using our baseline criteria A second implementation where the you strive to improve that implementation, according to any of the baseline criteria. Note that merely doing some existing standard data mining project will NOT be sufficcient Paper are 8+ pages pdf in ACM format The code review will be the lecturer reading the code checking that the implementation is above a minimum level of effort. Lecturer may call students/ teams at any time to his office for such reviews. Presentations will start mid-November Paper is not due till Dec 1. Note that: Master students will do the big project in groups of 3. All other work is solo. Submissions will be posted to the submit site . Take long urls and shorten them with (e.g.) tiny.cc before psiting.","title":"Grades"},{"location":"syllabus/#attendance","text":"Attendance is extremely important for your learning experience in this class. Once you reach three unexcused absences, each additional absence will reduce your attendance grade by 10%. Except for officially allowed reasons, your presence in the class if required from day one. Late-comers will have to work in their own solo groups (to avoid disruptions to existing groups). Note that absences for weddings (your own, or someone else's, is not an offically allowed reason). Exceptions: this subject will support students who are absent for any of the following officially allowed reasons: Anticipated Absences (cleared with the instructor before the absence). Examples of anticipated situations include representing an official university function, e.g., participating in a professional meeting, as part of a judging team, or athletic team; required court attendance as certified by the Clerk of Court; religious observances as verified by the Division of Academic and Student Affairs (DASA). Required military duty as certified by the student's commanding officer. Unanticipated Absences. Excuses must be reported to the instructor not more than one week after the return to class. Examples of unanticipated absences are: Short-term illness or injury affecting the ability to attend or to be productive academically while in class, or that could jeopardize the health of the individual or the health of the classmates attending. Students must notify instructors prior to the class absence, if possible, that they are temporarily unable to attend class or complete assignments on time. Death or serious illnesses in the family when documented appropriately. An attempt to verify deaths or serious illness will be made by the Division of Academic and Student Affairs. That support will include changing the schedule of deliverables and/or (in extreme case) different grading arrangements.","title":"Attendance"},{"location":"syllabus/#academic-integrity","text":"Cheating will be punished to the full extent permitted. Cheating includes plagerism of other people's work. All students will be working on public code repositories and informed reuse is encouraged where someone else's product is: Imported and clearly acknowledged (as to where it came from); The imported project is understood, and The imported project is significantly extended. Students are encouraged to read each others code and repor uninformed reuse to the lecturer. The issue will be explored and, if uncovered, cheating will be reported to the university and marks will be deducted if the person who is doing the reuse: Does not acknowledge the source of the product; Does not exhibit comprehension of the product when asked about it; Does not significantly extend the product. All students are expected to maintain traditional standards of academic integrity by giving proper credit for all work. All suspected cases of academic dishonesty will be aggressively pursued. You should be aware of the University policy on academic integrity found in the Code of Student Conduct. The exams will be done individually. Academic integrity is important. Do not work together on the exams: cheating on either will be punished to the full extent permitted.","title":"Academic Integrity"},{"location":"syllabus/#disabilities","text":"Reasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with Disability Services for Students at 1900 Student Health Center, Campus Box 7509, 919-515-7653. For more information on NC State's policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation(REG 02.20.01). Students are responsible for reviewing the PRRs which pertain to their course rights and responsibilities. These include: http://policies.ncsu.edu/policy/pol-04-25-05 (Equal Opportunity and Non-Discrimination Policy Statement), http://oied.ncsu.edu/oied/policies.php (Office for Institutional Equity and Diversity),http://policies.ncsu.edu/policy/pol-11-35-01 (Code of Student Conduct), and http://policies.ncsu.edu/regulation/reg-02-50-03 (Grades and Grade Point Average).","title":"Disabilities"},{"location":"syllabus/#non-discrimination-policy","text":"NC State University provides equality of opportunity in education and employment for all students and employees. Accordingly, NC State affirms its commitment to maintain a work environment for all employees and an academic environment for all students that is free from all forms of discrimination. Discrimination based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation is a violation of state and federal law and/or NC State University policy and will not be tolerated. Harassment of any person (either in the form of quid pro quo or creation of a hostile environment) based on race, color, religion, creed, sex, national origin, age, disability, veteran status, or sexual orientation also is a violation of state and federal law and/or NC State University policy and will not be tolerated. Note that, as a lecturer, I am legally required to report all such acts to the campus policy. Retaliation against any person who complains about discrimination is also prohibited. NC State's policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or http://www.ncsu.edu/equal_op/. Any person who feels that he or she has been the subject of prohibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148.","title":"Non-Discrimination Policy"},{"location":"syllabus/#other-information","text":"Non-scheduled class time for field trips or out-of-class activities are NOT required for this class. No such trips are currently planned. However, if they do happen then students are required to purchase liability insurance. For more information, see http://www2.acs.ncsu.edu/insurance/","title":"Other Information"},{"location":"lectures/","text":"","title":"Home"},{"location":"lectures/axe/","text":"Axe = argmin, argmax of X ArgMin(d,x,list) and ArgMax(d,x,list) are functions that seek the value of element a=list[i] that minimizes/maximizes the expected value of the function x(a) above and below that element. This process is then repeated recursviely d times on each part of the data. A large number of learners, clusterers, evaluation methods can be characeterised in this way. Example1 For example, given the numbers 100,10,90,20,110,30,90,40,120 we might what to know how to best split the numbers so as to minimize standard deviation. Step1: sort the numbers: all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 Step2: what is f of the whole space? all.n = 8 all.mu = 65 all.sd = sqrt( sum( (all[i] - all.mu)^2 ) / (n-1) ) = 44.4 Step3: find any split (say,i=2) and calculate the standard deviation above and below that slot left = all[ i = 2 ] right = all[ i 2 ] left.sd = 7.07 right.sd = 37.64 Step 4: find the expected value of the standard deviation afer the split. xpect(n, n1, v1, n2, v2) = n1/n*v1 + n2/n * v2 left.n = 2 right.n = 6 xpect(8, left.n, left.sd, right.n, right.sd) = 30.00 From this we can conclude that splitting at x=2 will reduce the uncertainty in this space from 44.4 to 30. Great! Step5: now repeat the above, but for all values of i . For sanity sake, we might demand that each left and right split not be too small; e.g. enough=2 items. We'll also assume there exists some Num class that can incrementally update and report n and sd . Here's ArgMin(1,sd, list) ; i.e. only one level of split and we want to minimize standard deviation: def sd(x, y): return xpect(x.n + y.n, x.n, x.sd, y.n, y.sd) def lt(x, y): return x y def gt(x, y): return x y def xSplit(lst, lo = 1, hi = length(lst), enough= 2, better= lt, f = sd, best = 10000000): cut = nil if hi-lo 2*enough: # otherwise, why bother? left = Num() right = Num() for i = lo, hi: right + lst[i] # give eveything to right for i = lo, hi: a = lst[i] left + a # add one to the right right - a # remove one from the left if left.n = enough and right.n = enough: tmp = f(left,right) if better( tmp , best ): cut, best = i, tmp return cut,best Note that: This function returns cut=nil if no cut is found. This function assumes the list is sorted before calling this function. For Python use lo,hi = 0 to length-1 ; If you want to generate more than one split, then call it recursively with lo=lo,hi=cut and lo=cut+1,hi=hi and best (in the recursive call) equal to the best return from the above). To minimze on other things then use another xpect function. This can actually serve as a argmax as well. Change better to gt Initialize best to a large negative number. If we called this in the above data, then it would recommend we cut at 4, i.e. the list all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 should split into left = 10,20,30,40 right= 90,100,110,120 So what we have right now is a very simple 1-dimensional clustering algorithm for a list of numeric numbers For lists of more complex data types, we need to pass in a selector, which we will call x that extracts the number we want to use from each item in the list. E.g. if clustering on an Employee's age, we might use x=employee.age . Further, given tables of independent and dependent variables, sometimes we want to clsuter on the independent variable in order to have most impact on the dependent variable. e.g. age alive sanity checks Sanit.g.y checks. sd cohen. fayyad iranni. is it valid to split (are the splits actually different?) scott knott FFTtrees which would have an expected value of standard deviation of 12.9 (much less than the origianl 44.4) def gotEnd(x, y) return xpect(x.n + y.n, x.n, x.ent, y.n, y.ent)","title":"Axe"},{"location":"lectures/axe/#axe-argmin-argmax-of-x","text":"ArgMin(d,x,list) and ArgMax(d,x,list) are functions that seek the value of element a=list[i] that minimizes/maximizes the expected value of the function x(a) above and below that element. This process is then repeated recursviely d times on each part of the data. A large number of learners, clusterers, evaluation methods can be characeterised in this way.","title":"Axe = argmin, argmax of X"},{"location":"lectures/axe/#example1","text":"For example, given the numbers 100,10,90,20,110,30,90,40,120 we might what to know how to best split the numbers so as to minimize standard deviation. Step1: sort the numbers: all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 Step2: what is f of the whole space? all.n = 8 all.mu = 65 all.sd = sqrt( sum( (all[i] - all.mu)^2 ) / (n-1) ) = 44.4 Step3: find any split (say,i=2) and calculate the standard deviation above and below that slot left = all[ i = 2 ] right = all[ i 2 ] left.sd = 7.07 right.sd = 37.64 Step 4: find the expected value of the standard deviation afer the split. xpect(n, n1, v1, n2, v2) = n1/n*v1 + n2/n * v2 left.n = 2 right.n = 6 xpect(8, left.n, left.sd, right.n, right.sd) = 30.00 From this we can conclude that splitting at x=2 will reduce the uncertainty in this space from 44.4 to 30. Great! Step5: now repeat the above, but for all values of i . For sanity sake, we might demand that each left and right split not be too small; e.g. enough=2 items. We'll also assume there exists some Num class that can incrementally update and report n and sd . Here's ArgMin(1,sd, list) ; i.e. only one level of split and we want to minimize standard deviation: def sd(x, y): return xpect(x.n + y.n, x.n, x.sd, y.n, y.sd) def lt(x, y): return x y def gt(x, y): return x y def xSplit(lst, lo = 1, hi = length(lst), enough= 2, better= lt, f = sd, best = 10000000): cut = nil if hi-lo 2*enough: # otherwise, why bother? left = Num() right = Num() for i = lo, hi: right + lst[i] # give eveything to right for i = lo, hi: a = lst[i] left + a # add one to the right right - a # remove one from the left if left.n = enough and right.n = enough: tmp = f(left,right) if better( tmp , best ): cut, best = i, tmp return cut,best Note that: This function returns cut=nil if no cut is found. This function assumes the list is sorted before calling this function. For Python use lo,hi = 0 to length-1 ; If you want to generate more than one split, then call it recursively with lo=lo,hi=cut and lo=cut+1,hi=hi and best (in the recursive call) equal to the best return from the above). To minimze on other things then use another xpect function. This can actually serve as a argmax as well. Change better to gt Initialize best to a large negative number. If we called this in the above data, then it would recommend we cut at 4, i.e. the list all= 10,20,30,40,90,100,110,120 i= 1 2 3 4 5 6 7 8 should split into left = 10,20,30,40 right= 90,100,110,120 So what we have right now is a very simple 1-dimensional clustering algorithm for a list of numeric numbers For lists of more complex data types, we need to pass in a selector, which we will call x that extracts the number we want to use from each item in the list. E.g. if clustering on an Employee's age, we might use x=employee.age . Further, given tables of independent and dependent variables, sometimes we want to clsuter on the independent variable in order to have most impact on the dependent variable. e.g. age alive sanity checks Sanit.g.y checks. sd cohen. fayyad iranni. is it valid to split (are the splits actually different?) scott knott FFTtrees which would have an expected value of standard deviation of 12.9 (much less than the origianl 44.4) def gotEnd(x, y) return xpect(x.n + y.n, x.n, x.ent, y.n, y.ent)","title":"Example1"},{"location":"lectures/baselines/","text":"Baseline for an \"Adequate\" AI Software engineering is about engineering and engineering is about generate a produce of adequate quality, given the available constraints. What does that mean for AI-enhanced software? Within any optimizer or data mining toolkit we can find hundreds of classifiers, regression tools, neural nets, support vector machines, evolutionary algorithms, ant-colony optimizers, etc etc, etc. These primitives can be combined in millions of ways, then tuned in quadrillions of ways (see the very active research literature on all these methods). So given a new problem, which learner/optimizer should we apply? This is a very hard problem. Wolpert reports in his famous No Free Lunch Theorems that if some optimizer/learner works best for some data, then some other optimizer/learner will work best for other data . This means that when new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. (Aside: it turns out that the NFL has some good news for us: the greater the performance gain desired, the fewer the learners exist that produce at least such a performance gain . See the Hyperband optimizer for an adaptive approach to pruning away less-than-great methods. Also, for many learners/optimizers, their performance is indistinguishable for anything less than some value. So if we divide the output space into bins of width means we can stop looking once we find a few methods that falls into the best bins .) When conducting such commissioning experiments, it is methodologically useful to have a baseline method ; i.e. an algorithm which can generate floor performance values. Such baselines let a developer quickly rule out any method that falls \u201cbelow the floor\u201d. With this, researchers and industrial practitioners can achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \u201ctry to beat the baseline\u201d). Using baselines for analyzing algorithms has been endorsed by several experienced researchers: In his textbook on empirical methods for artificial intelligence , Cohen strongly recommends comparing supposedly sophisticated systems against simpler alternatives. In the machine learning community, Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. In the software engineering community, Sarro et al et al. recently proposed baseline methods for effort estimation . Shepperd and Macdonnel argue convincingly that measurements are best viewed as ratios compared to measurements taken from some minimal baseline system . Work on cross-versus within-company cost estimation has also recommended the use of some very simple baseline I've offered several good baseline AI tools for SE tasks. In both the following, my graduate students were able to replace widely used very complex solutions with much simple alternative. For example: Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering So for this subject, we propose replacing the question of \"what AI tool is best\" with two other questions that make more sense to engineers racing to deliver products with limited time and resources: How can we quickly commission an initial, adequate, AI baseline system? What can we do to test and improve on that baseline? But what is a \"good\" baseline? Here's one list of what a \"baseline\" means. Items 1..10 are adapted from Sarro, TOSEM'18 . (which we extend with our own notes). The other items come from my experience. Note also that the following list offers a road map for future research in SE+AI. Find cases where some of these points do not matter. Find ways to enhance existing systems such that they perform better on the following criteria. Etc. The key thing to note about the following is that one system may not satisfy all these criteria (in fact, no known system satisfied all of them). That said, each of the following points is important. And by reflecting on the value of each point for a particular AI application, we naturally consider and review (and possibly discard) important design alternatives. The Checklist Now stay calm citizens of FSS'18. The following is not as complex as it looks. while there are many complex ways to support the following, there are also very simple ways that can work very well for each. And for your project, you only have to understand a one of the following. 1. SIMPLE: Be simple to describe, implement, and interpret (i.e. interpret the output for business uses). My current \"simplest\" methods is Fast-and-Frugal decision trees (or see also here ); which is available in a nice R-package . 2. REASONABLE: Offer comparable performance to standard methods. So note the great paradox of simplicity research Curious fact: evaluation can get so hard that we usually try to milk it for everything that can. So \"cross val\" experiments lead us to ensemble learning then to boosting ; \"round robin\" lead to transfer learning ; \"jiggle a little\" lead to evolutionary methods ; evolutionary methods. It is very hard to be simple Cause the simplest thing has to be compared against other, more complex, things. 3. STABLE: Be deterministic stable in its outcomes I've replaced deterministic with stable, since I think that is more important. Instability is very unsettling for software project managers struggling to find general policies. Project managers lose faith in the results of software analytics if those results keep changing. Also, such instability prevents project managers from offering clear guidelines on what to change, or what to avoid, in a project. And instability plagues SE data . For example: Here , Fig1, are the coefficients learned by regression on 20 67% samples of some training data. Note their WILD instability. One trick for increasing stability is not to focus on all the smaller details e.g. when learning regression equations, do not use all the variables; e.g. when learning rules, avoid wide conditions e.g. when learning trees, do not learn deep trees). Of course, if you optimize for simplicity, you may pay a performance penalty. 4a. INTUITIVE: Be applicable to mixed qualitative and quantitative data. It is good to use numeric and symbolic data. It is useful to be able to initial a systems with qualitative intuitions. Then, at least, you can compare the output to what folks already believe. But be warned, in SE, the beliefs of many developers are... dubious . If for no other reasons that humans have numerous cognitive biases For a really long list of those biases, see Wikipedia or this chart It is useful to be able to guide model construction via high-level qualitative goals. One useful technology here are Bayes nets which can be either initially drawn by people, then revised by data miners, or vice versa. Another trick is to use some incremental rule learning algorithm that updates many possible new rules, then scores them by their distance to old rules (and the best new rules are those that are closest to old and score highest). In that rig, user background beliefs would become the first generation rules. Yet another method is to use multi-objective optimizers that fit rule learner to human biases. 4b. COMPREHENSIBLE: This is connected to 4a. Essential for communities critiquing ideas. If the only person reading a model is a carburetor, then we can expect little push back. But if your models are about policies that humans have to implement, then I take it as axiomatic that humans will want to read and critique the models. 5. GENERAL: Offer some explanatory information regarding the prediction by representing generalized properties of the underlying data. Many systems offer only \"point\" solutions; i.e. examples of what might be useful. Given N attributes, a point solution offers exact values for all attributes. E.g. the output of most evolutionary programs E.g. all instance-based (nearest neighbor) methods E.g. A happy author might be editing this particular file and this particular time and place. Some systems offer solutions that hold over a volume; I.e. they ignore some values while saying things like x 10 for others. E.g. Happy authors might be editing html files on many computers (and when they do it does not matter). One way to generalize a instance-based method is to cluster the solutions, then only report ranges that are different in different clusters. 6. NO MAGIC: Have no magic parameters within the modeling process that require tuning. E.g. for random forests, engineers have to decide on how many trees are included in the forest. Alternatively, if such tunings exist, then the must be some automatic method for selecting what tunings are best for particular data sets. 7. AVAILABLE: Be publicly available via a reference implementation and associated environment for execution. In this day and age of Docker images and package managers and Github-like environments where everyone can load up each other's code at the drop of a hat, it makes no sense for some baseline tool to be inaccessible. 8. USEFUL: Generally be more accurate than a random guess or (e.g. an estimate based purely on the distribution of the response variable). E.g. evaluate the output via \"standardarized error\"; i.e. compare the prediction to some some prediction generated from (say) the median value of the response variable. 9. CHEAP: Do not be expensive to apply. Here we mean that the CPU, Ram, and disk space required to make something work is not crazy high. \"CHEAP is important since ~Reproducing and improving an old ideas means that you can reproduce that old result. Also, certifying that new ideas often means multiple runs over many sub-samples of the data. Such reproducibility and certification is impractical when such reproduction is impractically slow 10. ROBUST: I.e. does not change much over different data splits and validation methods? And if it does vary wildly, can it find ways to find regions in the data where the data conclusions are stable. 11. GOAL-AWARE: Different goals means different models. AND multiple goals = no problem! This is important since most data miners build models that optimizer for a single goal (e.g. minimize error or least-square error) yet business users often want their data miners to achieve many goals. For example, if we want to ask \"what to do\" rather than \"what is\", then we need a planner, not a classifier. Of course, in that case, the classifier can be used as a what-if guide to assess different plans. 12. CONTEXT-AWARE: context-aware: Easy path context awareness: first cluster the data, then build different models for different clusters: see NbTrees . Knows that local parts of data generate different models. E.g. hierarchically clusters the data and builds one model per cluster. While general principles are good, so too is how to handle particular contexts. For example, in general, exercise is good for maintaining healthy. However, in the particular context of patients who have just had cardiac surgery, then that general principle has to be carefully tailored to particular patients. ideas need to be updated. 13. HUMBLE: Easy path to certification envelopes: cluster data, report k items per cluster. Can publish succinct certification envelope that can report when new data is out-of-scope to what was seen before.(so we know when not to trust) This is important since the delivered data mined models should be able to recognize when new data is out-of-scope of anything they\u2019ve seen before. This means, at runtime, having access to the data used to build that model. Note that phrase succinct here: certification envelopes cannot include all the data relating to a model, otherwise every hard drive in the world will soon fill up. Another form of humility is knowing when the baseline should be replaced with something else. Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. 14. STREAMING: Can run over an infinite stream of data, updating itself (or knows when to go back to old versions of itself). Easy path to anomaly detection: cluster data, report items that fall far from each cluster. Can detect anomalies (when new inputs differ from old training data). This is the trigger for re-learning. 15. SHARABLE: Knows how to transfer models, data, between contexts. Easy path to lightweight sharing: just share reduced data from context awareness. Need some way to keep the volume of shared data down (otherwise \"sharing\" would clog the Internet). Such transfer may requires some transformation of the source data to the target data. 16. PRIVACY-AWARE: Easy path to privacy: within the clusters of the certification envelope, just share k items per cluster, each slightly mutated. See LACE2 . Can hide an individual's data This is essential when sharing a certification envelope Project The project of this class is to apply the above to AI tools applied to SE problems. Even trying to apply the above and not getting anywhere, would also be fine (just as you long as you document your comprehension of the ideas of baselines, along the way). So go seek, or build, good baselines: Take any SE problem and ask are the current methods \"baselines?\". Would simpler alternatives suffice? Can you make the method simpler to use; e.g. replace it with something much simpler to implement and explain e.g. apply an optimizer to a data mining to find better settings from that data miner? If you replace the complex with the simpler, what (if any) is the performance penalty? Can you make the method use less RAM or be faster to use; e.g. see what happens if you learn on just X% of the data (randomly selected) for X {50,25,10,5,1}%? If you apply a prototype generator, can you select/build a very small subset of the data from which learning is faster and just as effective? Finding prototypes can be as easy as \"cluster and take just a few from each cluster\" But there are many other methods eg. apply a data miner to an optimizer to divide up the data to make the whole process much faster? See 500+ faster than deep learning . Does that method need additional support to enable explanation of their output? Do their models fail the stability test? How does that method respond if you run it N times on 90% of the data? And if they do, can you find regions of the data where the performance is stable? How to reduce the CPU and RAM and runtime requirements of that method by large amounts e.g. see 500+ faster than deep learning . If we stream over the data, how soon does this model stabilize? If we inject mutations into the data, can this method be used to recognize that strange data? Once the weird data arrives, how long (if ever) before the model recovers? If a model is update, can be it done some minimally ; i.e. with least change to the existing model? etc etc etc All Connected The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomalous so the easier it is to offer a certification envelope Note that if our compression method is somehow hierarchical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, leaving the rest stable. Other Requirements No eval tools Tests conclusion stability across multiple data sets (if available) or across multiple subsets of know scenarios See Evaluation for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms. No support for stats tests Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"Baselines for Adequate AI &#10004;"},{"location":"lectures/baselines/#baseline-for-an-adequate-ai","text":"Software engineering is about engineering and engineering is about generate a produce of adequate quality, given the available constraints. What does that mean for AI-enhanced software? Within any optimizer or data mining toolkit we can find hundreds of classifiers, regression tools, neural nets, support vector machines, evolutionary algorithms, ant-colony optimizers, etc etc, etc. These primitives can be combined in millions of ways, then tuned in quadrillions of ways (see the very active research literature on all these methods). So given a new problem, which learner/optimizer should we apply? This is a very hard problem. Wolpert reports in his famous No Free Lunch Theorems that if some optimizer/learner works best for some data, then some other optimizer/learner will work best for other data . This means that when new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. (Aside: it turns out that the NFL has some good news for us: the greater the performance gain desired, the fewer the learners exist that produce at least such a performance gain . See the Hyperband optimizer for an adaptive approach to pruning away less-than-great methods. Also, for many learners/optimizers, their performance is indistinguishable for anything less than some value. So if we divide the output space into bins of width means we can stop looking once we find a few methods that falls into the best bins .) When conducting such commissioning experiments, it is methodologically useful to have a baseline method ; i.e. an algorithm which can generate floor performance values. Such baselines let a developer quickly rule out any method that falls \u201cbelow the floor\u201d. With this, researchers and industrial practitioners can achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \u201ctry to beat the baseline\u201d). Using baselines for analyzing algorithms has been endorsed by several experienced researchers: In his textbook on empirical methods for artificial intelligence , Cohen strongly recommends comparing supposedly sophisticated systems against simpler alternatives. In the machine learning community, Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. In the software engineering community, Sarro et al et al. recently proposed baseline methods for effort estimation . Shepperd and Macdonnel argue convincingly that measurements are best viewed as ratios compared to measurements taken from some minimal baseline system . Work on cross-versus within-company cost estimation has also recommended the use of some very simple baseline I've offered several good baseline AI tools for SE tasks. In both the following, my graduate students were able to replace widely used very complex solutions with much simple alternative. For example: Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering So for this subject, we propose replacing the question of \"what AI tool is best\" with two other questions that make more sense to engineers racing to deliver products with limited time and resources: How can we quickly commission an initial, adequate, AI baseline system? What can we do to test and improve on that baseline?","title":"Baseline for an \"Adequate\" AI"},{"location":"lectures/baselines/#but-what-is-a-good-baseline","text":"Here's one list of what a \"baseline\" means. Items 1..10 are adapted from Sarro, TOSEM'18 . (which we extend with our own notes). The other items come from my experience. Note also that the following list offers a road map for future research in SE+AI. Find cases where some of these points do not matter. Find ways to enhance existing systems such that they perform better on the following criteria. Etc. The key thing to note about the following is that one system may not satisfy all these criteria (in fact, no known system satisfied all of them). That said, each of the following points is important. And by reflecting on the value of each point for a particular AI application, we naturally consider and review (and possibly discard) important design alternatives.","title":"But what is a \"good\" baseline?"},{"location":"lectures/baselines/#the-checklist","text":"Now stay calm citizens of FSS'18. The following is not as complex as it looks. while there are many complex ways to support the following, there are also very simple ways that can work very well for each. And for your project, you only have to understand a one of the following. 1. SIMPLE: Be simple to describe, implement, and interpret (i.e. interpret the output for business uses). My current \"simplest\" methods is Fast-and-Frugal decision trees (or see also here ); which is available in a nice R-package . 2. REASONABLE: Offer comparable performance to standard methods. So note the great paradox of simplicity research Curious fact: evaluation can get so hard that we usually try to milk it for everything that can. So \"cross val\" experiments lead us to ensemble learning then to boosting ; \"round robin\" lead to transfer learning ; \"jiggle a little\" lead to evolutionary methods ; evolutionary methods. It is very hard to be simple Cause the simplest thing has to be compared against other, more complex, things. 3. STABLE: Be deterministic stable in its outcomes I've replaced deterministic with stable, since I think that is more important. Instability is very unsettling for software project managers struggling to find general policies. Project managers lose faith in the results of software analytics if those results keep changing. Also, such instability prevents project managers from offering clear guidelines on what to change, or what to avoid, in a project. And instability plagues SE data . For example: Here , Fig1, are the coefficients learned by regression on 20 67% samples of some training data. Note their WILD instability. One trick for increasing stability is not to focus on all the smaller details e.g. when learning regression equations, do not use all the variables; e.g. when learning rules, avoid wide conditions e.g. when learning trees, do not learn deep trees). Of course, if you optimize for simplicity, you may pay a performance penalty. 4a. INTUITIVE: Be applicable to mixed qualitative and quantitative data. It is good to use numeric and symbolic data. It is useful to be able to initial a systems with qualitative intuitions. Then, at least, you can compare the output to what folks already believe. But be warned, in SE, the beliefs of many developers are... dubious . If for no other reasons that humans have numerous cognitive biases For a really long list of those biases, see Wikipedia or this chart It is useful to be able to guide model construction via high-level qualitative goals. One useful technology here are Bayes nets which can be either initially drawn by people, then revised by data miners, or vice versa. Another trick is to use some incremental rule learning algorithm that updates many possible new rules, then scores them by their distance to old rules (and the best new rules are those that are closest to old and score highest). In that rig, user background beliefs would become the first generation rules. Yet another method is to use multi-objective optimizers that fit rule learner to human biases. 4b. COMPREHENSIBLE: This is connected to 4a. Essential for communities critiquing ideas. If the only person reading a model is a carburetor, then we can expect little push back. But if your models are about policies that humans have to implement, then I take it as axiomatic that humans will want to read and critique the models. 5. GENERAL: Offer some explanatory information regarding the prediction by representing generalized properties of the underlying data. Many systems offer only \"point\" solutions; i.e. examples of what might be useful. Given N attributes, a point solution offers exact values for all attributes. E.g. the output of most evolutionary programs E.g. all instance-based (nearest neighbor) methods E.g. A happy author might be editing this particular file and this particular time and place. Some systems offer solutions that hold over a volume; I.e. they ignore some values while saying things like x 10 for others. E.g. Happy authors might be editing html files on many computers (and when they do it does not matter). One way to generalize a instance-based method is to cluster the solutions, then only report ranges that are different in different clusters. 6. NO MAGIC: Have no magic parameters within the modeling process that require tuning. E.g. for random forests, engineers have to decide on how many trees are included in the forest. Alternatively, if such tunings exist, then the must be some automatic method for selecting what tunings are best for particular data sets. 7. AVAILABLE: Be publicly available via a reference implementation and associated environment for execution. In this day and age of Docker images and package managers and Github-like environments where everyone can load up each other's code at the drop of a hat, it makes no sense for some baseline tool to be inaccessible. 8. USEFUL: Generally be more accurate than a random guess or (e.g. an estimate based purely on the distribution of the response variable). E.g. evaluate the output via \"standardarized error\"; i.e. compare the prediction to some some prediction generated from (say) the median value of the response variable. 9. CHEAP: Do not be expensive to apply. Here we mean that the CPU, Ram, and disk space required to make something work is not crazy high. \"CHEAP is important since ~Reproducing and improving an old ideas means that you can reproduce that old result. Also, certifying that new ideas often means multiple runs over many sub-samples of the data. Such reproducibility and certification is impractical when such reproduction is impractically slow 10. ROBUST: I.e. does not change much over different data splits and validation methods? And if it does vary wildly, can it find ways to find regions in the data where the data conclusions are stable. 11. GOAL-AWARE: Different goals means different models. AND multiple goals = no problem! This is important since most data miners build models that optimizer for a single goal (e.g. minimize error or least-square error) yet business users often want their data miners to achieve many goals. For example, if we want to ask \"what to do\" rather than \"what is\", then we need a planner, not a classifier. Of course, in that case, the classifier can be used as a what-if guide to assess different plans. 12. CONTEXT-AWARE: context-aware: Easy path context awareness: first cluster the data, then build different models for different clusters: see NbTrees . Knows that local parts of data generate different models. E.g. hierarchically clusters the data and builds one model per cluster. While general principles are good, so too is how to handle particular contexts. For example, in general, exercise is good for maintaining healthy. However, in the particular context of patients who have just had cardiac surgery, then that general principle has to be carefully tailored to particular patients. ideas need to be updated. 13. HUMBLE: Easy path to certification envelopes: cluster data, report k items per cluster. Can publish succinct certification envelope that can report when new data is out-of-scope to what was seen before.(so we know when not to trust) This is important since the delivered data mined models should be able to recognize when new data is out-of-scope of anything they\u2019ve seen before. This means, at runtime, having access to the data used to build that model. Note that phrase succinct here: certification envelopes cannot include all the data relating to a model, otherwise every hard drive in the world will soon fill up. Another form of humility is knowing when the baseline should be replaced with something else. Holte uses the OneR baseline algorithm as a scout that runs ahead of a more complicated learners as a way to judge the complexity of up-coming tasks. 14. STREAMING: Can run over an infinite stream of data, updating itself (or knows when to go back to old versions of itself). Easy path to anomaly detection: cluster data, report items that fall far from each cluster. Can detect anomalies (when new inputs differ from old training data). This is the trigger for re-learning. 15. SHARABLE: Knows how to transfer models, data, between contexts. Easy path to lightweight sharing: just share reduced data from context awareness. Need some way to keep the volume of shared data down (otherwise \"sharing\" would clog the Internet). Such transfer may requires some transformation of the source data to the target data. 16. PRIVACY-AWARE: Easy path to privacy: within the clusters of the certification envelope, just share k items per cluster, each slightly mutated. See LACE2 . Can hide an individual's data This is essential when sharing a certification envelope","title":"The Checklist"},{"location":"lectures/baselines/#project","text":"The project of this class is to apply the above to AI tools applied to SE problems. Even trying to apply the above and not getting anywhere, would also be fine (just as you long as you document your comprehension of the ideas of baselines, along the way). So go seek, or build, good baselines: Take any SE problem and ask are the current methods \"baselines?\". Would simpler alternatives suffice? Can you make the method simpler to use; e.g. replace it with something much simpler to implement and explain e.g. apply an optimizer to a data mining to find better settings from that data miner? If you replace the complex with the simpler, what (if any) is the performance penalty? Can you make the method use less RAM or be faster to use; e.g. see what happens if you learn on just X% of the data (randomly selected) for X {50,25,10,5,1}%? If you apply a prototype generator, can you select/build a very small subset of the data from which learning is faster and just as effective? Finding prototypes can be as easy as \"cluster and take just a few from each cluster\" But there are many other methods eg. apply a data miner to an optimizer to divide up the data to make the whole process much faster? See 500+ faster than deep learning . Does that method need additional support to enable explanation of their output? Do their models fail the stability test? How does that method respond if you run it N times on 90% of the data? And if they do, can you find regions of the data where the performance is stable? How to reduce the CPU and RAM and runtime requirements of that method by large amounts e.g. see 500+ faster than deep learning . If we stream over the data, how soon does this model stabilize? If we inject mutations into the data, can this method be used to recognize that strange data? Once the weird data arrives, how long (if ever) before the model recovers? If a model is update, can be it done some minimally ; i.e. with least change to the existing model? etc etc etc","title":"Project"},{"location":"lectures/baselines/#all-connected","text":"The more we compress the smaller the memory and the faster we learn and the less we need to share (so more privacy). The more we understand the data's prototypes the more we know what is usual/ unusual so we more we know what is anomalous so the easier it is to offer a certification envelope Note that if our compression method is somehow hierarchical and if we track the errors seen by our learners in different subtrees then the more we know which parts of the model need revising (and which can stay the same). Which means we only make revisions to the parts that matter, leaving the rest stable.","title":"All Connected"},{"location":"lectures/baselines/#other-requirements","text":"","title":"Other Requirements"},{"location":"lectures/baselines/#no-eval-tools","text":"Tests conclusion stability across multiple data sets (if available) or across multiple subsets of know scenarios See Evaluation for many examples of that kind of evaluation. Note that these can significantly increase the computational cost of using learners. Hence, the need to faster , lighter AI algorithms.","title":"No eval tools"},{"location":"lectures/baselines/#no-support-for-stats-tests","text":"Check if this treatment has same effect as that treatment. Need at least two tests: significance and effect size I also think you need a third test; Something that clusters the treatments before the other tests are applied Reduces the number of other statistical tests. E.g the Scott-Knot test.","title":"No support for stats tests"},{"location":"lectures/deeplearning/","text":"Deep learning DP is a kind of neural net architecture where the outputs on N i become the inputs to net i+1. Deep learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. Compared to the conventional machine learning algorithms, deep learning methods are very good at exploring high-dimensional data. DL has received a lot of press lately. A. Lot. Of. Press. The computational requirements for this method are challenging. One NCSU grad student working at Google this summer was running 3 years of CPU per day (no joke) for his DL experiments. While that that is certainly an extreme example, it does illustrate how costly it can be to tune DL networks. Literally, costly. We costed out one project where 4 students would use commercial GPU clusters to run 20 experiments per week for 3 years. That work would add $1M to the cost of that grant. So it seem DL is the answer, what was the question? If we ask what kind of learners do we need, then work backwards from there, do we get to DL? Perhaps not. Wolpert reports in his famous No Free Lunch Theorems that if some learner works best for some data, then some other earner will work best for other data . Which means that when DL is not the one solution to all problems. Rather, it is one amongst many that we need to try. For example, suppose we don't need a classifier, but a planner. In that case, DL might become part of a larger system than uses some classifier as a what-if query device to test different plans. And if we want an explanation system, then we 'd have to use some symbolic method to run over the same examples as DL to build a second model (and its that model we can show to users). Of course, sometimes DL is necessary. Some domains have such frighteningly complex data that layers of neural networks are required to tease out the complexities of the data. But sometimes, DL is not necessary. And in that case, it seems silly to incur the computational cost of DL. So far, in SE, the case for the superiority of DL other other methods has not been made: Are Deep Neural Networks the Best Choice for Modeling Source Code? Easy over Hard: A Case Study on Deep Learning 500+ Faster than Deep Learning","title":"Deep learning &#10004;"},{"location":"lectures/deeplearning/#deep-learning","text":"DP is a kind of neural net architecture where the outputs on N i become the inputs to net i+1. Deep learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. Compared to the conventional machine learning algorithms, deep learning methods are very good at exploring high-dimensional data. DL has received a lot of press lately. A. Lot. Of. Press. The computational requirements for this method are challenging. One NCSU grad student working at Google this summer was running 3 years of CPU per day (no joke) for his DL experiments. While that that is certainly an extreme example, it does illustrate how costly it can be to tune DL networks. Literally, costly. We costed out one project where 4 students would use commercial GPU clusters to run 20 experiments per week for 3 years. That work would add $1M to the cost of that grant. So it seem DL is the answer, what was the question? If we ask what kind of learners do we need, then work backwards from there, do we get to DL? Perhaps not. Wolpert reports in his famous No Free Lunch Theorems that if some learner works best for some data, then some other earner will work best for other data . Which means that when DL is not the one solution to all problems. Rather, it is one amongst many that we need to try. For example, suppose we don't need a classifier, but a planner. In that case, DL might become part of a larger system than uses some classifier as a what-if query device to test different plans. And if we want an explanation system, then we 'd have to use some symbolic method to run over the same examples as DL to build a second model (and its that model we can show to users). Of course, sometimes DL is necessary. Some domains have such frighteningly complex data that layers of neural networks are required to tease out the complexities of the data. But sometimes, DL is not necessary. And in that case, it seems silly to incur the computational cost of DL. So far, in SE, the case for the superiority of DL other other methods has not been made: Are Deep Neural Networks the Best Choice for Modeling Source Code? Easy over Hard: A Case Study on Deep Learning 500+ Faster than Deep Learning","title":"Deep learning"},{"location":"lectures/eval/","text":"Evaluation XXX must show SA trueth isnt what we thought. t tif a example depends of goals onwhat we are willing to comprosmise Re-run on Multiple Samples e.g. cross-val Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5. e.g. round robin Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4 e.g. incremental validation. Divide into \" x \" buckets, Train on buckets 1..i, test on i+1 http://www.cs.le.ac.uk/people/llm11/publications/MinkuYaoICSE14.pdf e.g. moving validation. Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test. e.g. RRS (repeated random streaming) e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement BTW, Beyond \"Evaluation\" Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair Incremental learning SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Evaluation"},{"location":"lectures/eval/#evaluation","text":"XXX must show SA trueth isnt what we thought. t tif a example depends of goals onwhat we are willing to comprosmise","title":"Evaluation"},{"location":"lectures/eval/#re-run-on-multiple-samples","text":"","title":"Re-run on Multiple Samples"},{"location":"lectures/eval/#eg-cross-val","text":"Divide into \" x bins Test on one bin, train on the others Runs the risk of using future data to train for testing on the past Using combined with some stochastic re-orderings So \" M \" times, randomly rarrange order of data Then do an \" N \"-way cross val for each order Avoids \"order effects\" where the results are some quirky result based on the order of data colelction/generation. M=N=10 is common but I've never seen the point for more than M=N=5.","title":"e.g. cross-val"},{"location":"lectures/eval/#eg-round-robin","text":"Given N projects Train on N-1, test on the nth. e.g Github issue close time, Table4","title":"e.g. round robin"},{"location":"lectures/eval/#eg-incremental-validation","text":"Divide into \" x \" buckets, Train on buckets 1..i, test on i+1 http://www.cs.le.ac.uk/people/llm11/publications/MinkuYaoICSE14.pdf","title":"e.g. incremental validation."},{"location":"lectures/eval/#eg-moving-validation","text":"Divide into \" x \" buckets, learn on buckets i..i+n, test on i+n+1. eg. Krishna's K-test.","title":"e.g. moving validation."},{"location":"lectures/eval/#eg-rrs-repeated-random-streaming","text":"e.g. repeatedly stream over the data, each time using n% of the data selected at random Q: What \"n\"? A: Engineering judgement","title":"e.g. RRS (repeated random streaming)"},{"location":"lectures/eval/#btw-beyond-evaluation","text":"Evaluation can be so tedious and time-consuming that many researchers have asked if all that inference can be applied to improving the model: So \"evaluation\" becomes \"improvement\" or \"monitor and repair\" Cross val to ensembles to bagging to boosting Round robin to transfer learning Anomaly detection to repair","title":"BTW, Beyond \"Evaluation\""},{"location":"lectures/eval/#incremental-learning","text":"SAWTOOTH: Dumb as all hell Active learning uncertainty sampling certainty sampling Bayesian Parameter optimization (widely used) FLASH","title":"Incremental learning"},{"location":"lectures/explain/","text":"Explain from the fse fft paper from the swan paper from the EMSE paper ICML Workshop on Human Interpretability in Machine Learning","title":"Explanation"},{"location":"lectures/explain/#explain","text":"from the fse fft paper from the swan paper from the EMSE paper ICML Workshop on Human Interpretability in Machine Learning","title":"Explain"},{"location":"lectures/simple/","text":"Simpler. Please. Enough Inference, But Not Too Much Simplicity is the ultimate form of sophistication. -- Leonardo da Vinci Less, But Better -- Dieter Rams Are our AI tools designed \"good\"? Based on my experience with industrial developers, I would say perhaps not. According to Dieter Rams, good design: is innovative \u2013 The possibilities for progression are not, by any means, exhausted. Technological development is always offering new opportunities for original designs. But imaginative design always develops in tandem with improving technology, and can never be an end in itself. makes a product useful \u2013 A product is bought to be used. It has to satisfy not only functional, but also psychological and aesthetic criteria. Good design emphasizes the usefulness of a product whilst disregarding anything that could detract from it. is aesthetic \u2013 The aesthetic quality of a product is integral to its usefulness because products are used every day and have an effect on people and their well-being. Only well-executed objects can be beautiful. makes a product understandable \u2013 It clarifies the product\u2019s structure. Better still, it can make the product clearly express its function by making use of the user's intuition. At best, it is self-explanatory. is unobtrusive \u2013 Products fulfilling a purpose are like tools. They are neither decorative objects nor works of art. Their design should therefore be both neutral and restrained, to leave room for the user's self-expression. is honest \u2013 It does not make a product appear more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept. is long-lasting \u2013 It avoids being fashionable and therefore never appears antiquated. Unlike fashionable design, it lasts many years \u2013 even in today's throw away society. is thorough down to the last detail \u2013 Nothing must be arbitrary or left to chance. Care and accuracy in the design process show respect towards the consumer. is environmentally friendly \u2013 Design makes an important contribution to the preservation of the environment. It conserves resources and minimizes physical and visual pollution throughout the life cycle of the product. is as little design as possible \u2013 Less, but better \u2013 because it concentrates on the essential aspects, and the products are not burdened with non-essentials. Back to purity, back to simplicity. The Obvious Counter-argument Against \"Keep it Simple\" Yes, sometimes, complexity is necessary e.g. 2% optimizations of turbulent non-linear air-flows across a wing in the transonic range But when it ain\u2019t Needless complexity = is just silliness Much industrial success with very complex image processing based on deep learners that derive fascinating internal features uses layers of neural nets All good stuff But much SE inference exhibits a large effect where large variances are observed in the performance of the learned models. For such large learning problems, ever increasing exact inference is... silly. And there are many reasons to reflect on how not to do \"it\" simpler. Reasons for less Your next 15 weeks Fewer headaches Higher marks Because, Historically, Simpler is often Better PCA, 1901 Narrows: Amarel 1960s Prototypes: Chen 1975 Frames: Minsky, 1975 Min environments: DeKleer, 1986 Saturation: Horgan Mathur: 1980 Homogeneous propagation: Michael: 1981 Master variables: Crawford Baker, 1995 Clumps, Druzdel, 1997 Feature subset section, Kohavi, 1997, Back doors, Williams, 2002 Active learning: many people (2000+) Mathematically, models are either simple or poorly supported by the data: See here Because in SE, More Complex is often superfluous Data from Norman Fenton\u2019s Bayes nets discussing software defects = yes, no Given classes x,y then Fx, Fy is frequency of some ranges in x,y Log Odds Ratio = log(Fx/Fy ) If zero if no difference in x,y Results: Most variables do not contribute to determination of defects Data from Papakroni 's masters thesis TRAIN: Project 21 features onto first 2 components of PCA Recursively divide two dimensions (at median) Stopping a SQRT(N) In each leaf, replace N projects with median centroid TEST: Estimate = interpolate 2 near centroids Performs no worse, and sometimes better, than Random forests, NaiveBayes For more, see Data Mining for very busy people Speed speed speed Analytics = work flow to condense much low-value data down to a few diamonds. Fisher et al. survey of 16 industrial data scientists, Due to computational cost of analytics, \"the luxuries of interactivity, direct manipulation, and fast system response are gone\". Modern cloud-based analytics as a throwback to the 1960s\u2013 batch processing \"Fast iteration is key, but incompatible with the way jobs are submitted and processed in the cloud. It\u2019s frustrating to wait for hours, only to realize you need a slight tweak to your feature set.\" Less cost (local hardware, cloud services) The following are somewhat extreme examples. But suppose we could do the following tasks orders of magnitude faster. Just imagine what else could we use all that saved CPU for? 15 years of CPU, one FSE 2013 analytics paper, Wang et al/ Evaluate automatic tuning tools for code clone recognition tools $1.2million Cost or renting of commercial cloud hardware One project, 5 students, 3 years FYI: that would EAT and SWALLOW the standard budgets NSF gives researchers like me Three years of CPU/day Anonymous NCSU grad student tuning deep learning networks at XXXX for summer 2018 Support the edge Edge computing Move some computational towards the edge of the network to harness computational capabilities that are currently untapped in edge nodes, such as base stations, routers and switches Internet of Things Simple AI could make better use of billions and billions of low power devices, many of which are operating at very low power Less Energy Consumption Power off your phone Stare at it How long before you stop? Phone - power = silly From Green in Software Engineering : Present-day primary energy footprints of three business software apps add up to as much as 373 Peta Joules/ year. People claim they will pay more for a \"green\" product. nearly half of surveyed organizations design their business models on the basis of sustainability. Sustainability = new source of innovation, cost-cutting opportunity , mechanism for gaining competitive advantage. Less pollution Creating that Energy IT-related services now account for 2% of all global carbon emissions-- roughly the same as the aviation sector Simpler AI lets us breathe easier. The cloud runs on http://time.com/46777/your-data-is-dirty-the-carbon-price-of-cloud-computing/ . Simpler explanation Less generation of solutions Less confusion Verrappa and Letier : \"..for industrial problems, these algorithms generate (many) solutions (makes) understanding them and selecting one among them difficult and time consuming\" Simpler Customization We are already delivering software more complex than what people can manage (see fig1 and fig3). Many software systems have poorly chosen defaults. Hence, it is useful to seek better configurations. Van Aken et al. report that the default MySQL configurations in 2016 assume that it will be installed on a machine that has 160MB of RAM (which, at that time, was incorrect by, at least, an order of magnitude) Herodotou et al. show how standard settings for text mining appli- cations in Hadoop result in worst-case execution times. In the same vein, Jamshidi et al. reports for text mining applications on Apache Storm, the throughput achieved using the worst configuration is 480 times slower than the throughput achieved by the best configuration. Understanding, the configuration space of software systems with large configuration space, is challenging. Exploring more than just a handful of configurations is usually infeasible due to long benchmarking time Because we need a baseline Because better science needs better baselines: Empirical methods in AI : Supposedly newer more sophisticated methods should be baselined against a seemingly simpler alternative Warning: when I do that, I often find simpler is better. Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering Because better engineering needs better baselines: So many AI tools, so many ways to tune them, so many ways to combine them So given a new problem, which learner/optimizer should we apply? Hard to say, apriori. When new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. Comissioning needs baselines i.e. an algorithm which can generate floor performance values. Lets a developer quickly rule out any method that falls \u201cbelow the floor\u201d. Lets us achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \"try to beat the baseline\"). Other Quicker more effective training, experimentation Less to understand, faster to understand. Less to twiddle, less to explore Easier Reproducibility Large config spaces are the bane of reproducibility. You did not get what I got? Di you set the X to 2, Y to 3, and Z to 10,30,age/shoe size+temperature? Solutions more trust-able More understandable, more explainable, more I want to use. Solutions easier to apply Less to do, faster to do Cause its just good science If we do not know what we are doing, we are very clumsy; When we understand it better, we are more elegant; So use this test to check if your thinking is confused, or clarified: are your current methods very complex/ time-consuming? A Common Recipe (For Me, at Least) Row reduction: Replace N rows of data with M N exemplars e.g. cluster and report just some items per cluster Column reduction: Prune C columns of data with the sqrt(C) most influential columns Range reduction (in columns): Discretize numerics, but only cut into ranges in sub-ranges have less variance Range pruning If there is a class variable: only cut if the associated class ranges are less varied Else, cluster row/column/range data and only cut if associated ranges are less varied in different clusters. Cautions So that\u2019s it? Just find the few dimensions that matter, then stop?o Well... The reduced data space exists... but how to find it? Feature selection on raw data? Or may be synthesized (PCA, spectral, etc) Also: And the dimensions that matter NOW May not matter SOON So an agent that assumes low dimensionality (but fyi, we should be doing that with all data mining approaches, anyway). Will always be checking and revising their dimensional","title":"Simplicity &#10004;"},{"location":"lectures/simple/#simpler-please","text":"","title":"Simpler. Please."},{"location":"lectures/simple/#enough-inference-but-not-too-much","text":"Simplicity is the ultimate form of sophistication. -- Leonardo da Vinci Less, But Better -- Dieter Rams Are our AI tools designed \"good\"? Based on my experience with industrial developers, I would say perhaps not. According to Dieter Rams, good design: is innovative \u2013 The possibilities for progression are not, by any means, exhausted. Technological development is always offering new opportunities for original designs. But imaginative design always develops in tandem with improving technology, and can never be an end in itself. makes a product useful \u2013 A product is bought to be used. It has to satisfy not only functional, but also psychological and aesthetic criteria. Good design emphasizes the usefulness of a product whilst disregarding anything that could detract from it. is aesthetic \u2013 The aesthetic quality of a product is integral to its usefulness because products are used every day and have an effect on people and their well-being. Only well-executed objects can be beautiful. makes a product understandable \u2013 It clarifies the product\u2019s structure. Better still, it can make the product clearly express its function by making use of the user's intuition. At best, it is self-explanatory. is unobtrusive \u2013 Products fulfilling a purpose are like tools. They are neither decorative objects nor works of art. Their design should therefore be both neutral and restrained, to leave room for the user's self-expression. is honest \u2013 It does not make a product appear more innovative, powerful or valuable than it really is. It does not attempt to manipulate the consumer with promises that cannot be kept. is long-lasting \u2013 It avoids being fashionable and therefore never appears antiquated. Unlike fashionable design, it lasts many years \u2013 even in today's throw away society. is thorough down to the last detail \u2013 Nothing must be arbitrary or left to chance. Care and accuracy in the design process show respect towards the consumer. is environmentally friendly \u2013 Design makes an important contribution to the preservation of the environment. It conserves resources and minimizes physical and visual pollution throughout the life cycle of the product. is as little design as possible \u2013 Less, but better \u2013 because it concentrates on the essential aspects, and the products are not burdened with non-essentials. Back to purity, back to simplicity.","title":"Enough Inference, But Not Too Much"},{"location":"lectures/simple/#the-obvious-counter-argument-against-keep-it-simple","text":"Yes, sometimes, complexity is necessary e.g. 2% optimizations of turbulent non-linear air-flows across a wing in the transonic range But when it ain\u2019t Needless complexity = is just silliness Much industrial success with very complex image processing based on deep learners that derive fascinating internal features uses layers of neural nets All good stuff But much SE inference exhibits a large effect where large variances are observed in the performance of the learned models. For such large learning problems, ever increasing exact inference is... silly. And there are many reasons to reflect on how not to do \"it\" simpler.","title":"The Obvious Counter-argument Against \"Keep it Simple\""},{"location":"lectures/simple/#reasons-for-less","text":"","title":"Reasons for less"},{"location":"lectures/simple/#your-next-15-weeks","text":"Fewer headaches Higher marks","title":"Your next 15 weeks"},{"location":"lectures/simple/#because-historically-simpler-is-often-better","text":"PCA, 1901 Narrows: Amarel 1960s Prototypes: Chen 1975 Frames: Minsky, 1975 Min environments: DeKleer, 1986 Saturation: Horgan Mathur: 1980 Homogeneous propagation: Michael: 1981 Master variables: Crawford Baker, 1995 Clumps, Druzdel, 1997 Feature subset section, Kohavi, 1997, Back doors, Williams, 2002 Active learning: many people (2000+) Mathematically, models are either simple or poorly supported by the data: See here","title":"Because, Historically, Simpler is often Better"},{"location":"lectures/simple/#because-in-se-more-complex-is-often-superfluous","text":"Data from Norman Fenton\u2019s Bayes nets discussing software defects = yes, no Given classes x,y then Fx, Fy is frequency of some ranges in x,y Log Odds Ratio = log(Fx/Fy ) If zero if no difference in x,y Results: Most variables do not contribute to determination of defects Data from Papakroni 's masters thesis TRAIN: Project 21 features onto first 2 components of PCA Recursively divide two dimensions (at median) Stopping a SQRT(N) In each leaf, replace N projects with median centroid TEST: Estimate = interpolate 2 near centroids Performs no worse, and sometimes better, than Random forests, NaiveBayes For more, see Data Mining for very busy people","title":"Because in SE, More Complex is often superfluous"},{"location":"lectures/simple/#speed-speed-speed","text":"Analytics = work flow to condense much low-value data down to a few diamonds. Fisher et al. survey of 16 industrial data scientists, Due to computational cost of analytics, \"the luxuries of interactivity, direct manipulation, and fast system response are gone\". Modern cloud-based analytics as a throwback to the 1960s\u2013 batch processing \"Fast iteration is key, but incompatible with the way jobs are submitted and processed in the cloud. It\u2019s frustrating to wait for hours, only to realize you need a slight tweak to your feature set.\"","title":"Speed speed speed"},{"location":"lectures/simple/#less-cost-local-hardware-cloud-services","text":"The following are somewhat extreme examples. But suppose we could do the following tasks orders of magnitude faster. Just imagine what else could we use all that saved CPU for? 15 years of CPU, one FSE 2013 analytics paper, Wang et al/ Evaluate automatic tuning tools for code clone recognition tools $1.2million Cost or renting of commercial cloud hardware One project, 5 students, 3 years FYI: that would EAT and SWALLOW the standard budgets NSF gives researchers like me Three years of CPU/day Anonymous NCSU grad student tuning deep learning networks at XXXX for summer 2018","title":"Less cost (local hardware, cloud services)"},{"location":"lectures/simple/#support-the-edge","text":"Edge computing Move some computational towards the edge of the network to harness computational capabilities that are currently untapped in edge nodes, such as base stations, routers and switches Internet of Things Simple AI could make better use of billions and billions of low power devices, many of which are operating at very low power","title":"Support the edge"},{"location":"lectures/simple/#less-energy-consumption","text":"Power off your phone Stare at it How long before you stop? Phone - power = silly From Green in Software Engineering : Present-day primary energy footprints of three business software apps add up to as much as 373 Peta Joules/ year. People claim they will pay more for a \"green\" product. nearly half of surveyed organizations design their business models on the basis of sustainability. Sustainability = new source of innovation, cost-cutting opportunity , mechanism for gaining competitive advantage.","title":"Less Energy Consumption"},{"location":"lectures/simple/#less-pollution-creating-that-energy","text":"IT-related services now account for 2% of all global carbon emissions-- roughly the same as the aviation sector Simpler AI lets us breathe easier. The cloud runs on http://time.com/46777/your-data-is-dirty-the-carbon-price-of-cloud-computing/ .","title":"Less pollution Creating that Energy"},{"location":"lectures/simple/#simpler-explanation","text":"Less generation of solutions Less confusion Verrappa and Letier : \"..for industrial problems, these algorithms generate (many) solutions (makes) understanding them and selecting one among them difficult and time consuming\"","title":"Simpler explanation"},{"location":"lectures/simple/#simpler-customization","text":"We are already delivering software more complex than what people can manage (see fig1 and fig3). Many software systems have poorly chosen defaults. Hence, it is useful to seek better configurations. Van Aken et al. report that the default MySQL configurations in 2016 assume that it will be installed on a machine that has 160MB of RAM (which, at that time, was incorrect by, at least, an order of magnitude) Herodotou et al. show how standard settings for text mining appli- cations in Hadoop result in worst-case execution times. In the same vein, Jamshidi et al. reports for text mining applications on Apache Storm, the throughput achieved using the worst configuration is 480 times slower than the throughput achieved by the best configuration. Understanding, the configuration space of software systems with large configuration space, is challenging. Exploring more than just a handful of configurations is usually infeasible due to long benchmarking time","title":"Simpler Customization"},{"location":"lectures/simple/#because-we-need-a-baseline","text":"Because better science needs better baselines: Empirical methods in AI : Supposedly newer more sophisticated methods should be baselined against a seemingly simpler alternative Warning: when I do that, I often find simpler is better. Search-based SE: A Baseline Method For Search-Based Software Engineering Data mining: Bellwethers: A Baseline Method For Transfer Learning Optimizing: \"Sampling\"' as a Baseline Optimizer for Search-based Software Engineering Because better engineering needs better baselines: So many AI tools, so many ways to tune them, so many ways to combine them So given a new problem, which learner/optimizer should we apply? Hard to say, apriori. When new data arrives, you need commissioning experiments ; i.e. try a variety of techniques before you can find what words best for the local data. Comissioning needs baselines i.e. an algorithm which can generate floor performance values. Lets a developer quickly rule out any method that falls \u201cbelow the floor\u201d. Lets us achieve fast early results, while also gaining some guidance in all their subsequent experimentation (specifically: \"try to beat the baseline\").","title":"Because we need a baseline"},{"location":"lectures/simple/#other","text":"Quicker more effective training, experimentation Less to understand, faster to understand. Less to twiddle, less to explore Easier Reproducibility Large config spaces are the bane of reproducibility. You did not get what I got? Di you set the X to 2, Y to 3, and Z to 10,30,age/shoe size+temperature? Solutions more trust-able More understandable, more explainable, more I want to use. Solutions easier to apply Less to do, faster to do Cause its just good science If we do not know what we are doing, we are very clumsy; When we understand it better, we are more elegant; So use this test to check if your thinking is confused, or clarified: are your current methods very complex/ time-consuming?","title":"Other"},{"location":"lectures/simple/#a-common-recipe-for-me-at-least","text":"Row reduction: Replace N rows of data with M N exemplars e.g. cluster and report just some items per cluster Column reduction: Prune C columns of data with the sqrt(C) most influential columns Range reduction (in columns): Discretize numerics, but only cut into ranges in sub-ranges have less variance Range pruning If there is a class variable: only cut if the associated class ranges are less varied Else, cluster row/column/range data and only cut if associated ranges are less varied in different clusters.","title":"A Common Recipe (For Me, at Least)"},{"location":"lectures/simple/#cautions","text":"So that\u2019s it? Just find the few dimensions that matter, then stop?o Well... The reduced data space exists... but how to find it? Feature selection on raw data? Or may be synthesized (PCA, spectral, etc) Also: And the dimensions that matter NOW May not matter SOON So an agent that assumes low dimensionality (but fyi, we should be doing that with all data mining approaches, anyway). Will always be checking and revising their dimensional","title":"Cautions"},{"location":"lectures/tables/","text":"Different Learners for Different Data Let us start at the very beginning (a very good place to start). When you read you begin with A-B-C. When you mine, you begin with data. Different kinds of data miners work best of different kinds of data. Such data may be viewed as tables of examples: Tables have one column per feature and one row per example. The columns may be numeric (have numbers) or symbolic (contain discrete values). Also, some columns are goals (things we want to predict using the other columns). Finally, columns may contain missing values. For example, in text mining, where there is one column per word and one row per document, the columns contain many missing values (since not all words appear in all documents) and there may be hundreds of thousands of columns. While text mining applications can have many columns, Big Data applications can have any number of columns and millions to billions of rows. For such very large datasets, a complete analysis may be impossible. Hence, these might be sampled probabilistically (e.g., using the naive Bayesian algorithm discussed below). On the other hand, when there are very few rows, data mining may fail since there are too few examples to support summarization. For such sparse tables, k- nearest neighbors (kNN) may be best. kNN makes conclusions about new examples by looking at their neighborhood in the space of old examples. Hence, kNN only needs a few (or even only one) similar examples to make conclusions. If a table has no goal columns, then this is an unsupervised learning problem that might be addressed by (say) finding clusters of similar rows using, say, K- means or expectation maximization. An alternate approach, taken by the Apriori association rule learner, is to assume that every column is a goal and to look for what combinations of any values predict for any combination of any other. If a table has one goal, then this is a supervised learning problem where the task is to find combinations of values from the other columns that predict for the goal values. Note that for datasets with one discrete goal feature, it is common to call that goal the class of the dataset. For example, here is a table of data for a simple data mining problem: outlook, temp,humid,wind,play ----------------------------- sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no overcast, 83, 86, FALSE, yes rainy, 70, 96, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 65, 70, TRUE, no overcast, 64, 65, TRUE, yes sunny, 72, 95, FALSE, no sunny, 69, 70, FALSE, yes rainy, 75, 80, FALSE, yes sunny, 75, 70, TRUE, yes overcast, 72, 90, TRUE, yes overcast, 81, 75, FALSE, yes rainy, 71, 91, TRUE, no In this table, we are trying to predict for the goal of play?, given a record of the weather. Each row is one example where we did or did not play golf (and the goal of data mining is to find what weather predicts for playing golf). Note that temp and humidity are numeric columns and there are no missing values. Such simple tables are characterized by just a few columns and not many rows (say, dozens to thousands). Traditionally, such simple data mining problems have been explored by C4.5 and CART. However, with some clever sampling of the data, it is possible to scale these traditional learners to Big Data problems. Y = F(X) One way to look at a table of data is an example of some function that computes columns \" Y \" from input columns \" X \". Splits Another way to look at a table of data is as a source of Split s. Columns have ranges Most ranges are not interesting (not useful for decision making) So most columns are not interesting Standard lesson: need only sqrt(col) of the columns (and for text mining data, even fewer) Sym columns: Splits are solo simples or disjunctions Num columns: Splits can be found oh so many ways","title":"Tables"},{"location":"lectures/tables/#different-learners-for-different-data","text":"Let us start at the very beginning (a very good place to start). When you read you begin with A-B-C. When you mine, you begin with data. Different kinds of data miners work best of different kinds of data. Such data may be viewed as tables of examples: Tables have one column per feature and one row per example. The columns may be numeric (have numbers) or symbolic (contain discrete values). Also, some columns are goals (things we want to predict using the other columns). Finally, columns may contain missing values. For example, in text mining, where there is one column per word and one row per document, the columns contain many missing values (since not all words appear in all documents) and there may be hundreds of thousands of columns. While text mining applications can have many columns, Big Data applications can have any number of columns and millions to billions of rows. For such very large datasets, a complete analysis may be impossible. Hence, these might be sampled probabilistically (e.g., using the naive Bayesian algorithm discussed below). On the other hand, when there are very few rows, data mining may fail since there are too few examples to support summarization. For such sparse tables, k- nearest neighbors (kNN) may be best. kNN makes conclusions about new examples by looking at their neighborhood in the space of old examples. Hence, kNN only needs a few (or even only one) similar examples to make conclusions. If a table has no goal columns, then this is an unsupervised learning problem that might be addressed by (say) finding clusters of similar rows using, say, K- means or expectation maximization. An alternate approach, taken by the Apriori association rule learner, is to assume that every column is a goal and to look for what combinations of any values predict for any combination of any other. If a table has one goal, then this is a supervised learning problem where the task is to find combinations of values from the other columns that predict for the goal values. Note that for datasets with one discrete goal feature, it is common to call that goal the class of the dataset. For example, here is a table of data for a simple data mining problem: outlook, temp,humid,wind,play ----------------------------- sunny, 85, 85, FALSE, no sunny, 80, 90, TRUE, no overcast, 83, 86, FALSE, yes rainy, 70, 96, FALSE, yes rainy, 68, 80, FALSE, yes rainy, 65, 70, TRUE, no overcast, 64, 65, TRUE, yes sunny, 72, 95, FALSE, no sunny, 69, 70, FALSE, yes rainy, 75, 80, FALSE, yes sunny, 75, 70, TRUE, yes overcast, 72, 90, TRUE, yes overcast, 81, 75, FALSE, yes rainy, 71, 91, TRUE, no In this table, we are trying to predict for the goal of play?, given a record of the weather. Each row is one example where we did or did not play golf (and the goal of data mining is to find what weather predicts for playing golf). Note that temp and humidity are numeric columns and there are no missing values. Such simple tables are characterized by just a few columns and not many rows (say, dozens to thousands). Traditionally, such simple data mining problems have been explored by C4.5 and CART. However, with some clever sampling of the data, it is possible to scale these traditional learners to Big Data problems.","title":"Different Learners for Different Data"},{"location":"lectures/tables/#y-fx","text":"One way to look at a table of data is an example of some function that computes columns \" Y \" from input columns \" X \".","title":"Y = F(X)"},{"location":"lectures/tables/#splits","text":"Another way to look at a table of data is as a source of Split s. Columns have ranges Most ranges are not interesting (not useful for decision making) So most columns are not interesting Standard lesson: need only sqrt(col) of the columns (and for text mining data, even fewer) Sym columns: Splits are solo simples or disjunctions Num columns: Splits can be found oh so many ways","title":"Splits"},{"location":"proj/stats/","text":"# Stats.py from future import division,print_function import sys,random, argparse sys.dont_write_bytecode=True class o(): \"Anonymous container\" def init (i,**fields) : i.override(fields) def override(i,d): i. dict .update(d); return i def repr (i): d = i. dict name = i. class . name return name+'{'+' '.join([':%s %s' % (k,d[k]) for k in i.show()])+ '}' def show(i): return [k for k in sorted(i. dict .keys()) if not \"_\" in k] The=o(cohen=0.3, small=3, epsilon=0.01, width=50,lo=0,hi=100,conf=0.01,b=1000,a12=0.56) parser = argparse.ArgumentParser( description=\"Apply Scott-Knot test to data read from standard input\") p=parser.add_argument p(\"--demo\",default=False, action=\"store_true\") p(\"--cohen\", type=float, default=0.3, metavar='N', help=\"too small if delta less than N*std of the data)\") p(\"--small\",type=int, metavar=\"N\",default=3, help=\"too small if hold less than N items\") p(\"--epsilon\", type=float, default=0.01,metavar=\"N\", help=\"a range is too small of its hi - lo N\") p(\"--width\",type=int,default=50,metavar=\"N\", help=\"width of quintile display\") p(\"--text\",type=int,default=12,metavar=\"N\", help=\"width of text display\") p(\"--conf\", type=float, default=0.01,metavar=\"N\", help=\"bootstrap tests with confidence 1-n\") p(\"--a12\",type=float, default=0.56, metavar=\"N\", help=\"threshold for a12 test: disable,small,med,large=0,0.56,0.64,0.71\") p(\"--useA12\",default=False, metavar=\"N\", help=\"True if you want to use A12 instead of cliff's delta\") p(\"--latex\",default=False,metavar=\"N\", help=\"default is false and True for getting a latex table for the data\") p(\"--cdelta\",default=0.147,metavar=\"N\", help=\"value for cliff's delta to be considered not a small effect\") args = parser.parse_args() The.cohen = args.cohen The.small = args.small The.epsilon = args.epsilon The.conf = args.conf The.width = args.width + 0 The.a12 = args.a12 + 0 The.text = args.text + 0 The.latex = args.latex The.useA12 = args.useA12 The.cdelta = args.cdelta TODO try: opts, args = getopt.getopt(argv, \"hg:d\", [\"help\", \"grammar=\"]) 2 except getopt.GetoptError: 3 usage() 4 sys.exit(2) Analysis of Experimental Data This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory. Before we begin... Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc... Lesson Zero Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson One Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ]) rank , name , med , iqr 1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00 Lesson Two Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ]) rank , name , med , iqr 1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90 Lesson Three Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ]) rank , name , med , iqr 1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 Lesson Four Heh? Where's lesson four? Lesson Five Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00 Lesson Six Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]]) rank , name , med , iqr 1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00 Lesson Seven All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ]) rank , name , med , iqr 1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95 So, How to Rank? For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then.... Theory The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other. Step1: Visualization Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50) Xtile The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" . Step2: Check Medians The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5 Step3: Effect size An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502 Significance Tests Standard Utils Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f )) Standard Accumulator for Numbers Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)] Cliff's Delta def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two The A12 Effect Size Test As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ```` Non-Parametric Hypothesis Testing The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf Examples def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2) Saner Hypothesis Testing The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one Putting it All Together Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Stats.py"},{"location":"proj/stats/#analysis-of-experimental-data","text":"This page is about the non-parametric statistical tests. It is also a chance for us to discuss a little statistical theory.","title":"Analysis of Experimental Data"},{"location":"proj/stats/#before-we-begin","text":"Imagine the following example contain objective scores gained from different optimizers x1,x2,x3,x4,...etc . Which results are ranked one, two, three etc...","title":"Before we begin..."},{"location":"proj/stats/#lesson-zero","text":"Some differences are obvious def rdiv0(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,6, 7, 8, 9] ])","title":"Lesson Zero"},{"location":"proj/stats/#rank-name-med-iqr","text":"1 , x1 , 51 , 11 ( | ), 0.34, 0.49, 0.51, 0.51, 0.60 2 , x2 , 800 , 200 ( | ---- -- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-one","text":"Some similarities are obvious... def rdiv1(): rdivDemo([ [ x1 ,0.1, 0.2, 0.3, 0.4], [ x2 ,0.1, 0.2, 0.3, 0.4], [ x3 ,6, 7, 8, 9] ])","title":"Lesson One"},{"location":"proj/stats/#rank-name-med-iqr_1","text":"1 , x1 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x2 , 30 , 20 ( | ), 0.10, 0.20, 0.30, 0.30, 0.40 2 , x3 , 800 , 200 ( | ---- *-- ), 6.00, 7.00, 8.00, 8.00, 9.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-two","text":"Many results often clump into less-than-many ranks. def rdiv2(): rdivDemo([ [ x1 ,0.34, 0.49, 0.51, 0.6], [ x2 ,0.6, 0.7, 0.8, 0.9], [ x3 ,0.15, 0.25, 0.4, 0.35], [ x4 ,0.6, 0.7, 0.8, 0.9], [ x5 ,0.1, 0.2, 0.3, 0.4] ])","title":"Lesson Two"},{"location":"proj/stats/#rank-name-med-iqr_2","text":"1 , x5 , 30 , 20 (--- --- | ), 0.10, 0.20, 0.30, 0.30, 0.40 1 , x3 , 35 , 15 ( ---- - | ), 0.15, 0.25, 0.35, 0.35, 0.40 2 , x1 , 51 , 11 ( ------ -- ), 0.34, 0.49, 0.51, 0.51, 0.60 3 , x2 , 80 , 20 ( | ---- -- ), 0.60, 0.70, 0.80, 0.80, 0.90 3 , x4 , 80 , 20 ( | ---- *-- ), 0.60, 0.70, 0.80, 0.80, 0.90","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-three","text":"Some results even clump into one rank (the great null result). def rdiv3(): rdivDemo([ [ x1 ,101, 100, 99, 101, 99.5], [ x2 ,101, 100, 99, 101, 100], [ x3 ,101, 100, 99.5, 101, 99], [ x4 ,101, 100, 99, 101, 100] ])","title":"Lesson Three"},{"location":"proj/stats/#rank-name-med-iqr_3","text":"1 , x1 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x2 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00 1 , x3 , 10000 , 150 (------- | ),99.00, 99.50, 100.00, 101.00, 101.00 1 , x4 , 10000 , 100 (-------------- | ),99.00, 100.00, 100.00, 101.00, 101.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-four","text":"Heh? Where's lesson four?","title":"Lesson Four"},{"location":"proj/stats/#lesson-five","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv5(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x3 ,11,11,11]])","title":"Lesson Five"},{"location":"proj/stats/#rank-name-med-iqr_4","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x3 , 1100 , 0 (* | ),11.00, 11.00, 11.00, 11.00, 11.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-six","text":"Some things had better clump to one thing (sanity check for the ranker). def rdiv6(): rdivDemo([ [ x1 ,11,11,11], [ x2 ,11,11,11], [ x4 ,32,33,34,35]])","title":"Lesson Six"},{"location":"proj/stats/#rank-name-med-iqr_5","text":"1 , x1 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 1 , x2 , 1100 , 0 ( | ),11.00, 11.00, 11.00, 11.00, 11.00 2 , x4 , 3400 , 200 ( | - * ),32.00, 33.00, 34.00, 34.00, 35.00","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#lesson-seven","text":"All the above scales to succinct summaries of hundreds, thousands, millions of numbers def rdiv7(): rdivDemo([ [ x1 ] + [rand()**0.5 for _ in range(256)], [ x2 ] + [rand()**2 for _ in range(256)], [ x3 ] + [rand() for _ in range(256)] ])","title":"Lesson Seven"},{"location":"proj/stats/#rank-name-med-iqr_6","text":"1 , x2 , 25 , 50 (-- * -|--------- ), 0.01, 0.09, 0.25, 0.47, 0.86 2 , x3 , 49 , 47 ( ------ *| ------- ), 0.08, 0.29, 0.49, 0.66, 0.89 3 , x1 , 73 , 37 ( ------|- * --- ), 0.32, 0.57, 0.73, 0.86, 0.95","title":"rank ,         name ,    med   ,  iqr"},{"location":"proj/stats/#so-how-to-rank","text":"For the most part, we are concerned with very high-level issues that strike to the heart of the human condition: What does it mean to find controlling principles in the world? How can we find those principles better, faster, cheaper? But sometimes we have to leave those lofty heights to discuss more pragmatic issues. Specifically, how to present the results of an optimizer and, sometimes, how to compare and rank the results from different optimizers. Note that there is no best way, and often the way we present results depends on our goals, the data we are procesing, and the audience we are trying to reach. So the statistical methods discussed below are more like first-pass approximations to something you may have to change extensively, depending on the task at hand. In any case, in order to have at least one report that that you quickly generate, then....","title":"So, How to Rank?"},{"location":"proj/stats/#theory","text":"The test that one optimizer is better than another can be recast as four checks on the distribution of performance scores. Visualize the data, somehow. Check if the central tendency of one distribution is better than the other; e.g. compare their median values. Check the different between the central tendencies is not some small effect . Check if the distributions are significantly different ; The first step is very important. Stats should always be used as sanity checks on intuitions gained by other means. So look at the data before making, possibly bogus, inferences from it. For example, here are some charts showing the effects on a population as we apply more and more of some treatment. Note that the mean of the populations remains unchanged, yet we might still endorse the treatment since it reduces the uncertainty associated with each population. Note that 2 and 3 and 4 must be all be true to assert that one thing generates better numbers than another. For example, one bogus conclusion would be to just check median values (step2) and ignore steps3 and steps4. BAD IDEA . Medians can be very misleading unless you consider the overall distributions (as done in step3 and step4). (As an aside, note that the above requests a check for median , not mean . This is required since, all things considered, means do not mean much, especially for highly skewed distributions. For example, Bill Gates and 35 homeless people are in the same room. Their mean annual income is over a billion dollars each- which is a number that characterized neither Mr. Gates or the homeless people. On the other hand, the median income of that population is close to zero- which is a number that characterizes most of that population. ) In practice, step2,step3,step4 are listed in increasing order of effort (e.g. the bootstrap sample method discussed later in this subject is an example of step4, and this can take a while to compute). So pragmatically, it is useful to explore the above in the order step1 then step2 then step3 then step4 (and stopping along the way if any part fails). For example, one possible bogus inference would be to apply step4 without the step3 since if the small effect test fails, then the third significance test is misleading. For example, returning to the above distributions, note the large overlap in the top two curves in those plots. When distributions exhibit a very large overlap, it is very hard to determine if one is really different to the other. So large variances can mean that even if the means are better , we cannot really say that the values in one distribution are usually better than the other.","title":"Theory"},{"location":"proj/stats/#step1-visualization","text":"Suppose we had two optimizers which in a 10 repeated runs generated performance from two models: 1: def _tile2(): 2: def show(lst): 3: return xtile(lst,lo=0, hi=1,width=25, 4: show= lambda s:\" %3.2f\" % s) 5: print \"one\", show([0.21, 0.29, 0.28, 0.32, 0.32, 6: 0.28, 0.29, 0.41, 0.42, 0.48]) 7: print \"two\", show([0.71, 0.92, 0.80, 0.79, 0.78, 8: 0.9, 0.71, 0.82, 0.79, 0.98]) When faced with new data, always chant the following mantra: First visualize it to get some intuitions; Then apply some statistics to double check those intuitions. That is, it is strong recommended that, prior doing any statistical work, an analyst generates a visualization of the data. Percentile charts a simple way to display very large populations in very little space. For example, here are our results from one , displayed on a range from 0.00 to 1.00. one * --| , 0.28, 0.29, 0.32, 0.41, 0.48 two | -- * -- , 0.71, 0.79, 0.80, 0.90, 0.98 In this percentile chart, the 2nd and 3rd percentiles as little dashes left and right of the median value, shown with a \"*\" , (learner two 's 3rd percentile is so small that it actually disappears in this display). The vertical bar \"|\" shows half way between the display's min and max (in this case, that would be (0.0+1.00)/2= 0.50)","title":"Step1: Visualization"},{"location":"proj/stats/#xtile","text":"The advantage of percentile charts is that we can show a lot of data in very little space. For example, here's an example where the xtile Python function shows 2000 numbers on two lines: Quintiles divide the data into the 10th, 30th, 50th, 70th, 90th percentile. Dashes ( \"-\" ) mark the range (10,30)th and (70,90)th percentiles; White space marks the ranges (30,50)th and (50,70)th percentiles. Consider two distributions, of 1000 samples each: one shows square root of a rand() and the other shows the square of a rand() . 10: def _tile() : 11: import random 12: r = random.random 13: def show(lst): 14: return xtile(lst,lo=0, hi=1,width=25, 15: show= lambda s:\" %3.2f\" % s) 16: print \"one\", show([r()*0.5 for x in range(1000)]) 17: print \"two\", show([r()2 for x in range(1000)]) In the following quintile charts, we show these distributions: The range is 0 to 1. One line shows the square of 1000 random numbers; The other line shows the square root of 1000 random numbers; Note the brevity of the display: one -----| * --- , 0.32, 0.55, 0.70, 0.84, 0.95 two -- * |-------- , 0.01, 0.10, 0.27, 0.51, 0.85 As before, the median value, shown with a \"*\" ; and the point half-way between min and max (in this case, 0.5) is shown as a vertical bar \"|\" .","title":"Xtile"},{"location":"proj/stats/#step2-check-medians","text":"The median of a list is the middle item of the sorted values, if the list is of an odd size. If the list size is even, the median is the two values either side of the middle: def median(lst,ordered=False): lst = lst if ordered else sorted(lst) n = len(lst) p = n // 2 if (n % 2): return lst[p] p,q = p-1,p q = max(0,(min(q,n))) return (lst[p] + lst[q]) * 0.5","title":"Step2: Check Medians"},{"location":"proj/stats/#step3-effect-size","text":"An effect size test is a sanity check that can be summarizes as follows: Don't sweat the small stuff; I.e. ignore small differences between items in the samples. My preferred test for small effect has: a simple intuition; which makes no assumptions about (say) Gaussian assumptions; and which has a solid lineage in the literature. Such a test is Vargha and Delaney 's A12 statistic. The statistic was proposed in Vargha and Delaney's 2000 paper was endorsed in many places including in Acruci and Briad 's ICSE 2011 paper. After I describe it to you, you will wonder why anyone would ever want to use anything else. Given a performance measure seen in m measures of X and n measures of Y , the A12 statistics measures the probability that running algorithm X yields higher values than running another algorithm Y . Specifically, it counts how often we seen larger numbers in X than Y (and if the same numbers are found in both, we add a half mark): a12= #(X.i Y.j) / (n*m) + .5#(X.i == Y.j) / (n*m) According to Vargha and Delaney, a small, medium, large difference between two populations is: large if a12 is over 71%; medium if a12 is over 64%; small if a12 is 56%, or less. A naive version of this code is shown here in the ab12slow function. While simple to code, this ab12slow function runs in polynomial time (since for each item in lst1 , it runs over all of lst2 ): def _ab12(): def a12slow(lst1,lst2): more = same = 0.0 for x in sorted(lst1): for y in sorted(lst2): if x==y : same += 1 elif x y : more += 1 return (more + 0.5*same) / (len(lst1)*len(lst2)) random.seed(1) l1 = [random.random() for x in range(5000)] more = [random.random()*2 for x in range(5000)] l2 = [random.random() for x in range(5000)] less = [random.random()/2.0 for x in range(5000)] for tag, one,two in [( 1less ,l1,more), ( 1more ,more,less),( same ,l1,l2)]: t1 = msecs(lambda : a12(l1,less)) t2 = msecs(lambda : a12slow(l1,less)) print( \\n ,tag, \\n ,t1,a12(one,two)) print(t2, a12slow(one,two)) Note that the test code _ ab12 shows that our fast and slow method generate the same A12 score, but the fast way does so thousands of times faster. The following tests show runtimes for lists of 5000 numbers: experimemt msecs(fast) a12(fast) msecs(slow) a12(slow) 1less 13 0.257 9382 0.257 1more 20 0.868 9869 0.868 same 11 0,502 9937 0.502","title":"Step3: Effect size"},{"location":"proj/stats/#significance-tests","text":"","title":"Significance Tests"},{"location":"proj/stats/#standard-utils","text":"Didn't we do this before? Misc functions: rand = random.random any = random.choice seed = random.seed exp = lambda n: math.e**n ln = lambda n: math.log(n,math.e) g = lambda n: round(n,2) def median(lst,ordered=False): if not ordered: lst= sorted(lst) n = len(lst) p = n//2 if n % 2: return lst[p] q = p - 1 q = max(0,min(q,n)) return (lst[p] + lst[q])/2 def msecs(f): import time t1 = time.time() f() return (time.time() - t1) * 1000 def pairs(lst): Return all pairs of items i,i+1 from a list. last=lst[0] for i in lst[1:]: yield last,i last = i def xtile(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star return '('+''.join(out) + ), + pretty(what) def _tileX() : import random random.seed(1) nums = [random.random()**2 for _ in range(100)] print(xtile(nums,lo=0,hi=1.0,width=25,show= %5.2f ))","title":"Standard Utils"},{"location":"proj/stats/#standard-accumulator-for-numbers","text":"Note the lt method: this accumulator can be sorted by median values. Warning: this accumulator keeps all numbers. Might be better to use a bounded cache. class Num: An Accumulator for numbers def __init__(i,name,inits=[]): i.n = i.m2 = i.mu = 0.0 i.all=[] i._median=None i.name = name i.rank = 0 for x in inits: i.add(x) def s(i) : return (i.m2/(i.n - 1))**0.5 def add(i,x): i._median=None i.n += 1 i.all += [x] delta = x - i.mu i.mu += delta*1.0/i.n i.m2 += delta*(x - i.mu) def __add__(i,j): return Num(i.name + j.name,i.all + j.all) def quartiles(i): def p(x) : return float(g(xs[x])) i.median() xs = i.all n = int(len(xs)*0.25) return p(n) , p(2*n) , p(3*n) def median(i): if not i._median: i.all = sorted(i.all) i._median=median(i.all) return i._median def __lt__(i,j): return i.median() j.median() def spread(i): i.all=sorted(i.all) n1=i.n*0.25 n2=i.n*0.75 if len(i.all) = 1: return 0 if len(i.all) == 2: return i.all[1] - i.all[0] else: return i.all[int(n2)] - i.all[int(n1)]","title":"Standard Accumulator for Numbers"},{"location":"proj/stats/#cliffs-delta","text":"def cliffsDelta(lst1, lst2, **dull): Returns delta and true if there are more than 'dull' differences # if not dull: # dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004) size = True m, n = len(lst1), len(lst2) lst2 = sorted(lst2) j = more = less = 0 for repeats, x in runs(sorted(lst1)): while j = (n - 1) and lst2[j] x: j += 1 more += j*repeats while j = (n - 1) and lst2[j] == x: j += 1 less += (n - j)*repeats d = (more - less) / (m*n) if abs(d) The.cdelta: size = False return size def lookup_size(delta, dull): :type delta: float :type dull: dict, a dictionary of small, medium, large thresholds. delta = abs(delta) if delta = dull['small']: return False if dull['small'] delta dull['medium']: return True if dull['medium'] = delta dull['large']: return True if delta = dull['large']: return True def runs(lst): Iterator, chunks repeated values for j, two in enumerate(lst): if j == 0: one, i = two, 0 if one != two: yield j - i, one i = j one = two yield j - i + 1, two","title":"Cliff's Delta"},{"location":"proj/stats/#the-a12-effect-size-test","text":"As above def a12slow(lst1,lst2): how often is x in lst1 more than y in lst2? more = same = 0.0 for x in lst1: for y in lst2: if x == y : same += 1 elif x y : more += 1 x= (more + 0.5*same) / (len(lst1)*len(lst2)) return x def a12(lst1,lst2): how often is x in lst1 more than y in lst2? def loop(t,t1,t2): while t1.j t1.n and t2.j t2.n: h1 = t1.l[t1.j] h2 = t2.l[t2.j] h3 = t2.l[t2.j+1] if t2.j+1 t2.n else None if h1 h2: t1.j += 1; t1.gt += t2.n - t2.j elif h1 == h2: if h3 and h1 h3 : t1.gt += t2.n - t2.j - 1 t1.j += 1; t1.eq += 1; t2.eq += 1 else: t2,t1 = t1,t2 return t.gt*1.0, t.eq*1.0 #-------------------------- lst1 = sorted(lst1, reverse=True) lst2 = sorted(lst2, reverse=True) n1 = len(lst1) n2 = len(lst2) t1 = o(l=lst1,j=0,eq=0,gt=0,n=n1) t2 = o(l=lst2,j=0,eq=0,gt=0,n=n2) gt,eq= loop(t1, t1, t2) return gt/(n1*n2) + eq/2/(n1*n2) = The.a12 def _a12(): def f1(): return a12slow(l1,l2) def f2(): return a12(l1,l2) for n in [100,200,400,800,1600,3200,6400]: l1 = [rand() for _ in xrange(n)] l2 = [rand() for _ in xrange(n)] t1 = msecs(f1) t2 = msecs(f2) print(n, g(f1()),g(f2()),int((t1/t2))) n a12(fast) a12(slow) tfast / tslow 100 0.53 0.53 4 200 0.48 0.48 6 400 0.49 0.49 28 800 0.5 0.5 26 1600 0.51 0.51 72 3200 0.49 0.49 109 6400 0.5 0.5 244 ````","title":"The A12 Effect Size Test"},{"location":"proj/stats/#non-parametric-hypothesis-testing","text":"The following bootstrap method was introduced in 1979 by Bradley Efron at Stanford University. It was inspired by earlier work on the jackknife. Improved estimates of the variance were developed later . To check if two populations (y0,z0) are different, many times sample with replacement from both to generate (y1,z1), (y2,z2), (y3,z3) .. etc. def sampleWithReplacement(lst): returns a list same size as list def any(n) : return random.uniform(0,n) def one(lst): return lst[ int(any(len(lst))) ] return [one(lst) for _ in lst] Then, for all those samples, check if some testStatistic in the original pair hold for all the other pairs. If it does more than (say) 99% of the time, then we are 99% confident in that the populations are the same. In such a bootstrap hypothesis test, the some property is the difference between the two populations, muted by the joint standard deviation of the populations. def testStatistic(y,z): Checks if two means are different, tempered by the sample size of 'y' and 'z' tmp1 = tmp2 = 0 for y1 in y.all: tmp1 += (y1 - y.mu)**2 for z1 in z.all: tmp2 += (z1 - z.mu)**2 s1 = (float(tmp1)/(y.n - 1))**0.5 s2 = (float(tmp2)/(z.n - 1))**0.5 delta = z.mu - y.mu if s1+s2: delta = delta/((s1/y.n + s2/z.n)**0.5) return delta The rest is just details: Efron advises to make the mean of the populations the same (see the yhat,zhat stuff shown below). The class total is a just a quick and dirty accumulation class. For more details see the Efron text . def bootstrap(y0,z0,conf=The.conf,b=The.b): The bootstrap hypothesis test from p220 to 223 of Efron's book 'An introduction to the boostrap. class total(): quick and dirty data collector def __init__(i,some=[]): i.sum = i.n = i.mu = 0 ; i.all=[] for one in some: i.put(one) def put(i,x): i.all.append(x); i.sum +=x; i.n += 1; i.mu = float(i.sum)/i.n def __add__(i1,i2): return total(i1.all + i2.all) y, z = total(y0), total(z0) x = y + z tobs = testStatistic(y,z) yhat = [y1 - y.mu + x.mu for y1 in y.all] zhat = [z1 - z.mu + x.mu for z1 in z.all] bigger = 0.0 for i in range(b): if testStatistic(total(sampleWithReplacement(yhat)), total(sampleWithReplacement(zhat))) tobs: bigger += 1 return bigger / b conf","title":"Non-Parametric Hypothesis Testing"},{"location":"proj/stats/#examples","text":"def _bootstraped(): def worker(n=1000, mu1=10, sigma1=1, mu2=10.2, sigma2=1): def g(mu,sigma) : return random.gauss(mu,sigma) x = [g(mu1,sigma1) for i in range(n)] y = [g(mu2,sigma2) for i in range(n)] return n,mu1,sigma1,mu2,sigma2,\\ 'different' if bootstrap(x,y) else 'same' # very different means, same std print(worker(mu1=10, sigma1=10, mu2=100, sigma2=10)) # similar means and std print(worker(mu1= 10.1, sigma1=1, mu2= 10.2, sigma2=1)) # slightly different means, same std print(worker(mu1= 10.1, sigma1= 1, mu2= 10.8, sigma2= 1)) # different in mu eater by large std print(worker(mu1= 10.1, sigma1= 10, mu2= 10.8, sigma2= 1)) Output: #_bootstraped() (1000, 10, 10, 100, 10, 'different') (1000, 10.1, 1, 10.2, 1, 'same') (1000, 10.1, 1, 10.8, 1, 'different') (1000, 10.1, 10, 10.8, 1, 'same') Warning- the above took 8 seconds to generate since we used 1000 bootstraps. As to how many bootstraps are enough, that depends on the data. There are results saying 200 to 400 are enough but, since I am suspicious man, I run it for 1000. Which means the runtimes associated with bootstrapping is a significant issue. To reduce that runtime, I avoid things like an all-pairs comparison of all treatments (see below: Scott-knott). Also, BEFORE I do the boostrap, I first run the effect size test (and only go to bootstrapping in effect size passes: def different(l1,l2): #return bootstrap(l1,l2) and a12(l2,l1) #return a12(l2,l1) and bootstrap(l1,l2) if The.useA12: return a12(l2,l1) and bootstrap(l1,l2) else: return cliffsDelta(l1,l2) and bootstrap(l1,l2)","title":"Examples"},{"location":"proj/stats/#saner-hypothesis-testing","text":"The following code, which you should use verbatim does the following: All treatments are clustered into ranks . In practice, dozens of treatments end up generating just a handful of ranks. The numbers of calls to the hypothesis tests are minimized: Treatments are sorted by their median value. Treatments are divided into two groups such that the expected value of the mean values after the split is minimized; Hypothesis tests are called to test if the two groups are truly difference. All hypothesis tests are non-parametric and include (1) effect size tests and (2) tests for statistically significant numbers; Slow bootstraps are executed if the faster A12 tests are passed; In practice, this means that the hypothesis tests (with confidence of say, 95%) are called on only a logarithmic number of times. So... With this method, 16 treatments can be studied using less than 1,2,4,8,16 log 2 i =15 hypothesis tests and confidence 0.99 15 =0.86 . But if did this with the 120 all-pairs comparisons of the 16 treatments, we would have total confidence _0.99 120 =0.30. For examples on using this code, see rdivDemo (below). def scottknott(data,cohen=The.cohen,small=The.small,useA12=The.a12 0, epsilon=The.epsilon): Recursively split data, maximizing delta of the expected value of the mean before and after the splits. Reject splits with under 3 items all = reduce(lambda x,y:x+y,data) same = lambda l,r: abs(l.median() - r.median()) = all.s()*cohen if useA12: same = lambda l, r: not different(l.all,r.all) big = lambda n: n small return rdiv(data,all,minMu,big,same,epsilon) def rdiv(data, # a list of class Nums all, # all the data combined into one num div, # function: find the best split big, # function: rejects small splits same, # function: rejects similar splits epsilon): # small enough to split two parts Looks for ways to split sorted data, Recurses into each split. Assigns a 'rank' number to all the leaf splits found in this way. def recurse(parts,all,rank=0): Split, then recurse on each part. cut,left,right = maybeIgnore(div(parts,all,big,epsilon), same,parts) if cut: # if cut, rank right higher than left rank = recurse(parts[:cut],left,rank) + 1 rank = recurse(parts[cut:],right,rank) else: # if no cut, then all get same rank for part in parts: part.rank = rank return rank recurse(sorted(data),all) return data def maybeIgnore((cut,left,right), same,parts): if cut: if same(sum(parts[:cut],Num('upto')), sum(parts[cut:],Num('above'))): cut = left = right = None return cut,left,right def minMu(parts,all,big,epsilon): Find a cut in the parts that maximizes the expected value of the difference in the mean before and after the cut. Reject splits that are insignificantly different or that generate very small subsets. cut,left,right = None,None,None before, mu = 0, all.mu for i,l,r in leftRight(parts,epsilon): if big(l.n) and big(r.n): n = all.n * 1.0 now = l.n/n*(mu- l.mu)**2 + r.n/n*(mu- r.mu)**2 if now before: before,cut,left,right = now,i,l,r return cut,left,right def leftRight(parts,epsilon=The.epsilon): Iterator. For all items in 'parts', return everything to the left and everything from here to the end. For reasons of efficiency, take a first pass over the data to pre-compute and cache right-hand-sides rights = {} n = j = len(parts) - 1 while j 0: rights[j] = parts[j] if j n: rights[j] += rights[j+1] j -=1 left = parts[0] for i,one in enumerate(parts): if i 0: if parts[i]._median - parts[i-1]._median epsilon: yield i,left,rights[i] left += one","title":"Saner Hypothesis Testing"},{"location":"proj/stats/#putting-it-all-together","text":"Driver for the demos: def rdivDemo(data,latex = True): def zzz(x): return int(100 * (x - lo) / (hi - lo + 0.00001)) data = map(lambda lst:Num(lst[0],lst[1:]), data) print( ) ranks=[] for x in scottknott(data,useA12=True): ranks += [(x.rank,x.median(),x)] all=[] for _,__,x in sorted(ranks): all += x.all all = sorted(all) lo, hi = all[0], all[-1] line = ---------------------------------------------------- last = None formatStr = '%%4s , %%%ss , %%s , %%4s ' % The.text # print((formatStr % \\ # ('rank', 'name', 'med', 'iqr')) + \\n + line) if latex: latexPrint(ranks,all) for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() print((formatStr % \\ (x.rank+1, x.name, q2, q3 - q1)) + \\ xtile(x.all,lo=lo,hi=hi,width=30,show= %5.2f )) last = x.rank def _rdivs(): seed(1) rdiv0(); rdiv1(); rdiv2(); rdiv3(); rdiv5(); rdiv6(); print( ### ); rdiv7() def latexPrint(ranks,all): print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%Latex Table%%%%%%%%%%%%%%%%%%%%%%%%% ) print( \\\\begin{figure}[!t] {\\small {\\small \\\\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} \\\\arrayrulecolor{darkgray} \\\\rowcolor[gray]{.9} rank treatment median IQR \\\\\\\\ ) lo, hi = all[0], all[-1] for _,__,x in sorted(ranks): q1,q2,q3 = x.quartiles() q1,q2,q3 = q1*100,q2*100,q3*100 print( %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\\\\\ % (x.rank+1,x.name.replace('_','\\_'),q2,q3-q1,q1,q2-q1,q3,q3-q2)) last = x.rank print( \\end{tabular}} } \\\\caption{%%%Enter Caption%%% }\\\\label{fig:my fig} \\\\end{figure} ) print( %%%%%%%%%%%%%%%%%%%%%%%End Latex Table%%%%%%%%%%%%%%%%%%%%% ) print( %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ) def xtile_modified(lst,lo=The.lo,hi=The.hi,width=The.width, chops=[0.1 ,0.3,0.5,0.7,0.9], marks=[ - , , , - , ], bar= | ,star= * ,show= %3.0f ): The function _xtile_ takes a list of (possibly) unsorted numbers and presents them as a horizontal xtile chart (in ascii format). The default is a contracted _quintile_ that shows the 10,30,50,70,90 breaks in the data (but this can be changed- see the optional flags of the function). def pos(p) : return ordered[int(len(lst)*p)] def place(x) : return int(width*float((x - lo))/(hi - lo+0.00001)) def pretty(lst) : return ', '.join([show % x for x in lst]) ordered = sorted(lst) lo = min(lo,ordered[0]) hi = max(hi,ordered[-1]) what = [pos(p) for p in chops] where = [place(n) for n in what] out = [ ] * width for one,two in pairs(where): for i in range(one,two): out[i] = marks[0] marks = marks[1:] out[int(width/2)] = bar out[place(pos(0.5))] = star print(what) return what #################################### def thing(x): Numbers become numbers; every other x is a symbol. try: return int(x) except ValueError: try: return float(x) except ValueError: return x def main(): log=None latex = The.latex all={} now=[] for line in sys.stdin: for word in line.split(): word = thing(word) if isinstance(word,str): now = all[word] = all.get(word,[]) else: now += [word] rdivDemo( [ [k] + v for k,v in all.items() ],latex) if args.demo: _rdivs() else: main() #print( \\begin{figure}[!t] # #{\\small # #{\\small \\begin{tabular}{l@{~~~}l@{~~~}r@{~~~}r@{~~~}c} #\\arrayrulecolor{darkgray} #\\rowcolor[gray]{.9} rank treatment median IQR #%min= 20, max= 117 #\\\\ + # %d %s %d %d \\quart{%d}{%d}{%d}{%d} \\\\ #+ \\end{tabular}} # #% :learn 4.64 :analyze 1.69 :boots 3 effects 5 :conf 0.970299 # #} #\\caption{COCOMO vs just lines #of code. SE values seen in #leave-one-studies, repeated ten times. #For each of the four tables in this figure, #{\\em better} methods appear {\\em higher} in the tables. #In these tables, #median and IQR are the 50th and the #(75-25)th percentiles. The IQR range is #shown in the right column #with black dot at the median. Horizontal lines #divide the ``ranks'' found by our Scott-Knott+bootstrapping+effect size tests (shown in left column). #}\\label{fig:loc} #\\end{figure} )","title":"Putting it All Together"},{"location":"proj/w1/","text":"Homework Week1, Week2 Todo Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet. A Simple Unit Test Rig (in Python) import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f Test rig, in action Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code). Task1 (week1): Python101 Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page). Task2 (week2): Sample Table Data (that we want to read) Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no'] Functions The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere() Test cases def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"One &#10004;"},{"location":"proj/w1/#homework-week1-week2","text":"","title":"Homework Week1, Week2"},{"location":"proj/w1/#todo","text":"Create a public Github repo (NOT in NC State Github, but in the other one) Add \"timm\" as a team member to that repo. How? go the repo's organization's settings on left-hand-side menu go to Collaberators and teams then Enter \"timm\" under \"Collaborators\". Start a file with the following header (containing class O ). For Week1, address the Python101 task. For Week2, address the Table reader task. Commit the code (w1.py, w2.py) and a transcript of the output (called w1.txt, w2.txt) to a sub-directory in your repo called w12 . Paste a link to that directory in the commit sheet.","title":"Todo"},{"location":"proj/w1/#a-simple-unit-test-rig-in-python","text":"import re,traceback class O: y=n=0 @staticmethod def report(): print( \\n# pass= %s fail= %s %%pass = %s%% % ( O.y,O.n, int(round(O.y*100/(O.y+O.n+0.001))))) @staticmethod def k(f): try: print( \\n-----| %s |----------------------- % f.__name__) if f.__doc__: print( # + re.sub(r'\\n[ \\t]*', \\n# ,f.__doc__)) f() print( # pass ) O.y += 1 except: O.n += 1 print(traceback.format_exc()) return f","title":"A Simple Unit Test Rig (in Python)"},{"location":"proj/w1/#test-rig-in-action","text":"Functions are called as a side-effect of load the file. The function comment is something the above rig prints out. If assertions fail, it prints the error but keeps on going to run the other tests. @O.k def testingFailure(): this one must fail.. just to test if the unit test system is working assert 1==2 @O.k def testingSuccess(): if this one fails, we have a problem! assert 1==1 if __name__== __main__ : O.report() For example, if you load this file with python3 thisfile.py you will see -----| testingFailure |----------------------- # this one must fail.. just to # test if the unit test system is working Traceback (most recent call last): File \"w1.py\", line 29, in k f() File \"w1.py\", line 52, in testingFailure assert 1==2 AssertionError -----| testingSuccess |----------------------- # if this one fails, we have a problem! # pass # pass= 1 fail= 1 %pass = 50% Note the last line (number of passes and failes in the code).","title":"Test rig, in action"},{"location":"proj/w1/#task1-week1-python101","text":"Read Basic Python Write 27 functions like testingSuccess (above) that demonstrate you understand that the code on pages 5 to 33, skipping p21 (so one function for one thing on each page).","title":"Task1 (week1): Python101"},{"location":"proj/w1/#task2-week2-sample-table-data-that-we-want-to-read","text":"Suppose we need to read in a table. DATA1 = outlook,$temp,?humidity,windy,play sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Then we need to learn the type of the data (on row1) which in this case is numeric (if name has \" $ \"); ignrore this column (if name has \" \"); string, otherwise. And some tables of data are more challenging that others. Here's one where there are comments (after a \" # \"); rows can continue onto the next line (if they end in \",\"); there can be blank lines in the file DATA2 = outlook, # weather forecast. $temp, # degrees farenheit ?humidity, # relative humidity windy, # wind is high play # yes,no sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64, 65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny, 75,70,TRUE,yes overcast,100,25,90,TRUE,yes overcast,81,75,FALSE,yes # unique day rainy,71,91,TRUE,no Regardless of those details, when we read both these strings, we see as output ['outlook', '$temp', 'windy', 'play'] ['sunny', 85.0, 'FALSE', 'no'] ['sunny', 80.0, 'TRUE', 'no'] ['overcast', 83.0, 'FALSE', 'yes'] ['rainy', 70.0, 'FALSE', 'yes'] ['rainy', 68.0, 'FALSE', 'yes'] ['rainy', 65.0, 'TRUE', 'no'] ['overcast', 64.0, 'TRUE', 'yes'] ['sunny', 72.0, 'FALSE', 'no'] ['sunny', 69.0, 'FALSE', 'yes'] ['rainy', 75.0, 'FALSE', 'yes'] ['sunny', 75.0, 'TRUE', 'yes'] ['overcast', 100.0, '90', 'TRUE'] ['overcast', 81.0, 'FALSE', 'yes'] ['rainy', 71.0, 'TRUE', 'no']","title":"Task2 (week2): Sample Table Data (that we want to read)"},{"location":"proj/w1/#functions","text":"The following functions implement the table reader. def lines(s): Return contents, one line at a time. yourCodeHere() def rows(src): Kill bad characters. If line ends in ',' then join to next. Skip blank lines. yourCodeHere() def cols(src): If a column name on row1 contains '?', then skip over that column. yourCodeHere() def prep(src): If a column name on row1 contains '$', coerce strings in that column to a float. yourCodeHere()","title":"Functions"},{"location":"proj/w1/#test-cases","text":"def ok0(s): for row in prep(cols(rows(lines(s)))): print(row) @O.k def ok1(): ok0(DATA1) @O.k def ok2(): ok0(DATA2)","title":"Test cases"}]}